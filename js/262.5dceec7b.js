"use strict";(self["webpackChunkpersonal_website"]=self["webpackChunkpersonal_website"]||[]).push([[262],{4262:function(e,t,n){n.r(t),t["default"]="## 1. What is an Environment in RL?\n\nIn reinforcement learning (RL), an environment is the external system in which an agent operates and interacts. But how do we formally define this concept? An environment in RL can be represented as a tuple:\n\n$$ \\mathcal{E} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{R}, \\mathcal{P}) $$\n\nWhere:\n- $\\mathcal{S}$: Set of all possible states\n- $\\mathcal{A}$: Set of all possible actions\n- $\\mathcal{R}$: Reward function\n- $\\mathcal{P}$: Transition probability function\n\nHow does this environment definition relate to agents? Agents are:\n\n$$ \\mathcal{A} = (S, A, \\pi, \\gamma, L) $$\n\nThe environment $\\mathcal{E}$ defines the rules and dynamics, while the agent $\\mathcal{A}$ defines the decision-making entity. The agent operates within the constraints set by the environment.\n\nNow that we understand the basic structure of an environment, let's delve deeper into each component.\n\n## 2. States: The Snapshot of the Environment\n\nWhat exactly is a state in RL? The state space $\\mathcal{S}$ represents all possible situations the environment can be in:\n\n$$ \\mathcal{S} = \\{s_1, s_2, s_3, \\ldots\\} $$\n\nEach state $s \\in \\mathcal{S}$ provides a complete description of the environment at a particular moment in time. It includes all the information necessary for the agent to make a decision.\n\nBut this raises an important question: Can the agent always perceive the complete state of the environment?\n\nI think s in S should be s_i in S right?\n\nCan I get an example or analogy for the difference between state and environment?\n\nIs state basically the full environment at a particular time t?\n\n## 3. Observations: The Agent's Window to the World\n\nIn many real-world scenarios, the agent cannot directly access the full state of the environment. Instead, it receives an observation $o$, which is partial information about the environment's state:\n\n$$ o = f(s), \\text{ where } o \\subseteq s \\text{ and } s \\in \\mathcal{S} $$\n\nHere, $f$ is a function that maps the true state to an observation, potentially losing information in the process.\n\nThis distinction between states and observations leads us to an important concept in RL: the difference between the true environment and the agent's perception.\n\nWhy is o lowercase? Is it because just like we have s_i in S we have o_i in O where O is a subset of S?\n\nMake the function consistent with full RL defn.\n\n## 4. The True Environment vs. Agent's Perception\n\nHow does what the agent perceives differ from the actual state of the environment? This concept is similar to the distinction between $y$ (true value) and $\\hat{y}$ (estimated value) in supervised learning or statistics.\n\n### True Environment (y):\n$$ \\mathcal{E} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{R}, \\mathcal{P}) $$\n\n### Agent's Perception (Å·):\n- Observations: $o = f(s)$, where $s \\in \\mathcal{S}$\n- Learned World Model: $M = (\\hat{\\mathcal{S}}, \\mathcal{A}, \\hat{\\mathcal{R}}, \\hat{\\mathcal{P}})$\n\nThe agent must learn and make decisions based on its observations and learned model, which may not perfectly reflect the true environment.\n\n\nThis could use an analogy, example, more explanation.\n\nBut how does the agent interact with this environment over time?\n\n## 5. Actions: The Agent's Toolkit\n\nThe action space $\\mathcal{A}$ represents all possible actions an agent can take:\n\n$$ \\mathcal{A} = \\{a_1, a_2, a_3, \\ldots\\} $$\n\nActions can be:\n1. Discrete: $\\mathcal{A} = \\{a_1, a_2, a_3, \\ldots\\}$\n2. Continuous: $\\mathcal{A} = \\{a \\in \\mathbb{R}^n \\mid a_{\\text{min}} \\leq a \\leq a_{\\text{max}}\\}$\n\nThe nature of the action space significantly impacts how the agent learns and makes decisions. But what happens after the agent takes an action?\n\n## 6. Transitions: The Dynamics of the Environment\n\nThe transition probability function $\\mathcal{P}$ defines how the environment changes in response to the agent's actions:\n\n$$ \\mathcal{P}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1] $$\n\nIt gives the probability of moving from one state to another given an action. Transitions can be:\n\n1. Deterministic: $s_{t+1} = f(s_t, a_t)$\n2. Stochastic: $s_{t+1} \\sim \\mathcal{P}(\\cdot | s_t, a_t)$\n\nHere, $s_{t+1}$ (also sometimes denoted as $s'$) represents the next state after taking action $a_t$ in state $s_t$.\n\nBut how does the agent know if its actions are good or bad?\n\nWhy are transitions probabilistic?\n\n## 7. Rewards: The Feedback Mechanism\n\nThe reward function $\\mathcal{R}$ provides feedback to the agent about its actions:\n\n$$ \\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R} $$\n\nIt assigns a numerical value to the transition from one state to another after taking an action:\n\n$$ r_t = \\mathcal{R}(s_t, a_t, s_{t+1}) $$\n\nThis feedback is crucial for the agent to learn which actions are beneficial and which are not.\n\nNow that we understand the basic components of an environment, how do they work together over time?\n\n## 8. Trajectory and History: Capturing the Agent's Journey\n\nAs the agent interacts with the environment, it generates a sequence of states, actions, and rewards. This sequence can be viewed from two perspectives:\n\n### Trajectory:\nA trajectory $\\tau$ represents a specific sequence of true states and actions:\n$$ \\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T) $$\n\n### History:\nHistory $h_t$ captures the sequence of observations, actions, and rewards up to time $t$:\n$$ h_t = (o_0, a_0, r_1, o_1, a_1, r_2, \\ldots, o_t) $$\n\nThe key difference is that trajectory deals with true states (often unknown to the agent), while history deals with observations (what the agent actually perceives).\n\nIs this distinction between trajectory and history actually true? Or how these words are commonly used in literature?\n\nBut how does the past influence the future in RL environments?\n\n# memory\n\nIf we have a history, we need some way to represent what we/the agent remembers and what it doesn't. The agent can internally keep a track of the history or just infer it from the environment through observation.\n\n## 9. The Markov Property and Its Implications\n\nThe Markov property states that the future depends only on the current state, not on the history:\n\n$$ P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_1, a_1, s_2, a_2, \\ldots, s_t, a_t) $$\n\nThis property leads to two important types of decision processes:\n\nWhat is a decision process?\n\n\n### Markov Decision Processes (MDPs):\nIn fully observable MDPs, the current observation (which equals the true state) is sufficient for optimal decision-making.\n\n### Partially Observable Markov Decision Processes (POMDPs):\nIn POMDPs, the true state is Markovian, but the observations are not. The agent must use history to infer the current state, often through a belief state:\n\n$$ b(s) = P(s | h_t) $$\n\nHow does this distinction affect the agent's decision-making process?\n\n## 10. The Role of History in Different Settings\n\n### In MDPs:\nTheoretically, the current state is sufficient, and history is not needed for optimal decision-making. However, history can still be useful for learning and updating the policy or value function.\n\n### In POMDPs:\nHistory is crucial for maintaining a belief state and making informed decisions. The agent uses history to approximate the true state:\n\n$$ \\hat{s}_t = g(h_t) $$\n\nWhere $g$ is a function that estimates the current state based on history, often implemented through recurrent neural networks or other memory mechanisms.\n\nBut how can an agent use its experiences to build a model of the environment?\n\n## 11. World Models: Bridging the Gap\n\nThe world model $M = (\\hat{\\mathcal{S}}, \\mathcal{A}, \\hat{\\mathcal{R}}, \\hat{\\mathcal{P}})$ is the agent's learned approximation of the true environment $\\mathcal{E}$.\n\n### Learning the World Model:\nThe agent uses its history of interactions to learn the world model:\n\n$$ \\hat{\\mathcal{P}}(s_{t+1}|s_t,a_t) \\approx \\frac{\\text{count}(s_t, a_t, s_{t+1})}{\\text{count}(s_t, a_t)} $$\n$$ \\hat{\\mathcal{R}}(s_t, a_t) \\approx \\frac{\\sum_{(s_t, a_t, r_t) \\in \\text{history}} r_t}{\\text{count}(s_t, a_t)} $$\n\n### Using the World Model:\nThe learned model allows the agent to simulate trajectories and plan actions without interacting with the true environment.\n\nHow do all these components come together in the agent's decision-making process?\n\n## 12. Connecting the Dots: From Observations to Decision-Making\n\n1. The agent receives an observation $o_t = f(s_t)$ of the true state $s_t$.\n2. It updates its history: $h_t = h_{t-1} \\cup (o_t, a_{t-1}, r_t)$.\n3. In POMDPs, it updates its belief state: $b(s) = P(s | h_t)$.\n4. It uses its policy $\\pi$, which may depend on the history or belief state, to choose an action: $a_t = \\pi(h_t)$ or $a_t = \\pi(b(s))$.\n5. The environment transitions to a new true state according to $\\mathcal{P}(s_{t+1}|s_t, a_t)$.\n6. The cycle repeats, generating a trajectory in the true environment and a history from the agent's perspective.\n\n## The Interplay of Environments and Agents\n\nUnderstanding RL environments involves grasping the distinction between the true environment and the agent's perception and learning. The concepts of states, observations, actions, transitions, rewards, history, and trajectories are all interconnected, each playing a crucial role in how the agent learns and makes decisions in the face of uncertainty and partial observability.\n\nDifferent types of agents (value-based, policy-based, model-based) are designed to excel in different types of environments (fully observable, partially observable, deterministic, stochastic). The environment provides the context, rules, and feedback, while the agent develops strategies to maximize its performance within these constraints.\n\nBy understanding these connections, we can see how the framework of RL environments provides the foundation for developing intelligent agents capable of learning and adapting in complex, dynamic settings.\n\n"}}]);
//# sourceMappingURL=262.5dceec7b.js.map