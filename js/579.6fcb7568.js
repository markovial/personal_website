"use strict";(self["webpackChunkpersonal_website"]=self["webpackChunkpersonal_website"]||[]).push([[579],{1579:function(n,e,a){a.r(e),e["default"]="- [[#1. What is an RL Agent?|1. What is an RL Agent?]]\n- [[#2. The Agent's Perspective: State Space|2. The Agent's Perspective: State Space]]\n- [[#3. The Agent's Toolkit: Action Space|3. The Agent's Toolkit: Action Space]]\n- [[#4. The Agent's Strategy: Policy|4. The Agent's Strategy: Policy]]\n- [[#5. Planning for the Future: Discount Factor|5. Planning for the Future: Discount Factor]]\n- [[#6. The Learning Process|6. The Learning Process]]\n- [[#7. Feedback Mechanism: Reward|7. Feedback Mechanism: Reward]]\n- [[#8. Measuring Agent Performance|8. Measuring Agent Performance]]\n- [[#9. Estimating Long-Term Value: Value Functions|9. Estimating Long-Term Value: Value Functions]]\n- [[#10. Types of RL Agents|10. Types of RL Agents]]\n\t- [[#10. Types of RL Agents#10.1 Model-Based Agents|10.1 Model-Based Agents]]\n\t- [[#10. Types of RL Agents#10.2 Model-Free Agents|10.2 Model-Free Agents]]\n\t- [[#10. Types of RL Agents#10.3 Value-Based Agents|10.3 Value-Based Agents]]\n\t- [[#10. Types of RL Agents#10.4 Policy-Based Agents|10.4 Policy-Based Agents]]\n\t- [[#10. Types of RL Agents#10.5 Actor-Critic Agents|10.5 Actor-Critic Agents]]\n\t- [[#10. Types of RL Agents#10.6 On-Policy Agents|10.6 On-Policy Agents]]\n\t- [[#10. Types of RL Agents#10.7 Off-Policy Agents|10.7 Off-Policy Agents]]\n- [[#11. Improving Agent Performance|11. Improving Agent Performance]]\n\n## 1. What is an RL Agent?\n\nAn RL agent is an entity that interacts with an environment to learn optimal behavior. But what exactly constitutes an agent? Let's break it down mathematically:\n\n$$ A = (S, A, \\pi, \\gamma) $$\n\nWhere:\n- $S$: State Space\n- $A$: Action Space\n- $\\pi$: Policy (learned)\n- $\\gamma$: Discount Factor\n\nThis tuple represents the core components of an agent. But what do each of these mean, and how do they work together?\n\n## 2. The Agent's Perspective: State Space\n\nThe state space $S$ is the set of all possible situations the agent might encounter:\n\n$$ S = \\{s_1, s_2, s_3, \\ldots, s_n\\} $$\n\nEach $s_i$ represents a unique state. Think of it as the agent's perception of its environment at any given moment. For example, in a chess game, a state would be the current configuration of all pieces on the board.\n\nBut how does the agent interact with its environment based on these states?\n\n## 3. The Agent's Toolkit: Action Space\n\nThe action space $A$ is the set of all possible actions the agent can take:\n\n$$ A = \\{a_1, a_2, a_3, \\ldots, a_m\\} $$\n\nEach $a_j$ represents a unique action. Continuing our chess example, an action would be moving a specific piece to a specific square.\n\nNow that we know what the agent can perceive (states) and what it can do (actions), how does it decide which action to take in each state?\n\n## 4. The Agent's Strategy: Policy\n\nThe policy $\\pi$ is the agent's strategy, mapping states to actions:\n\n$\\pi: S \\rightarrow A$\n\nIt can be:\n1. Deterministic: $a_t = \\mu(s_t)$\n2. Stochastic: $a_t \\sim \\pi(\\cdot | s_t)$\n\nA deterministic policy always chooses the same action for a given state, while a stochastic policy assigns probabilities to actions.\n\nBut why would an agent choose one action over another? This brings us to the concept of rewards and long-term planning.\n\n## 5. Planning for the Future: Discount Factor\n\nThe discount factor $\\gamma$ determines how much the agent values future rewards compared to immediate ones:\n\n$$ 0 \\leq \\gamma \\leq 1 $$\n\nA higher $\\gamma$ means the agent is more forward-thinking, valuing future rewards more heavily. This is crucial because in many scenarios, the best long-term strategy might involve sacrificing immediate rewards for greater future gains.\n\nNow that we understand the basic components of an agent, how does it actually learn and improve its behavior?\n\n## 6. The Learning Process\n\nAgents learn through interaction with their environment. This process is guided by a learning algorithm (L). We can expand our agent tuple to include this:\n\n$$ A = (S, A, \\pi, \\gamma, L) $$\n\nBut what does it mean for an agent to \"learn\"? In essence, learning in RL typically involves updating the policy $\\pi$ or estimating value functions (which we'll discuss later). This often translates to adjusting parameters in neural networks or other function approximators.\n\nDifferent types of agents use different learning algorithms. For example, in Q-learning:\n\n$$ Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(s, a)] $$\n\nThis update rule adjusts the estimated value of taking action $a$ in state $s$ based on the reward received and the estimated value of the next state.\n\nBut how does the agent know if it's doing well or poorly? This brings us to the concept of rewards.\n\n## 7. Feedback Mechanism: Reward\n\nTo learn, agents need feedback. This comes in the form of a reward function:\n\n$$ R: S \\times A \\times S \\rightarrow \\mathbb{R} $$\n\nThe reward function assigns a numerical value to the transition from one state to another after taking an action:\n\n$$ r_t = R(s_t, a_t, s_{t+1}) $$\n\nThis is how the environment communicates to the agent whether its actions are good or bad.\n\nGiven this feedback mechanism, how do we measure the overall performance of an agent?\n\n## 8. Measuring Agent Performance\n\nWe typically measure agent performance by the expected cumulative reward:\n\n$$ \\text{Performance}(A) = \\mathbb{E} \\left[ \\sum_{t=0}^{T} \\gamma^t R_t \\right] $$\n\nWe use the expected value because the environment and/or the agent's policy might be stochastic, leading to different outcomes even with the same starting conditions. We sum the rewards because we're interested in the agent's performance over time, not just in a single moment.\n\nBut how does the agent estimate the long-term value of its actions? This brings us to the concept of value functions.\n\n## 9. Estimating Long-Term Value: Value Functions\n\nAgents often learn value functions to estimate the long-term reward of being in a state or taking an action:\n\n1. State-Value Function: $V: S \\rightarrow \\mathbb{R}$\n   $$ V(s) = \\mathbb{E}[G_t | S_t = s] = \\mathbb{E} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Bigg| S_t = s \\right] $$\n\n2. Action-Value Function: $Q: S \\times A \\rightarrow \\mathbb{R}$\n   $$ Q(s, a) = \\mathbb{E}[G_t | S_t = s, A_t = a] = \\mathbb{E} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Bigg| S_t = s, A_t = a \\right] $$\n\nThe key difference is that the state-value function $V(s)$ estimates the expected return from being in state $s$ and following the current policy thereafter, while the action-value function $Q(s,a)$ estimates the expected return from taking action $a$ in state $s$ and then following the current policy.\n\nNow that we understand these fundamental concepts, let's explore different types of RL agents and how they approach the learning problem.\n\n## 10. Types of RL Agents\n\nRL algorithms can be categorized in several ways. One fundamental distinction is between model-based and model-free methods:\n\n### 10.1 Model-Based Agents\n\nThese agents learn a model of their environment:\n\n$$ A_{mb} = (S, A, \\hat{R}, \\hat{P}, \\pi: S \\rightarrow A, \\gamma, L_{mb}, PA) $$\n\nWhere:\n- $\\hat{R}: S \\times A \\times S \\rightarrow \\mathbb{R}$: Estimated Reward Function\n- $\\hat{P}: S \\times A \\times S \\rightarrow [0, 1]$: Estimated Transition Probability Function\n- $PA$: Planning Algorithm\n\nModel-based agents try to learn how the environment works ($\\hat{P}$ and $\\hat{R}$) and use this knowledge to plan their actions.\n\n### 10.2 Model-Free Agents\n\nThese agents learn directly from experience without building an explicit model:\n\n$$ A_{mf} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{mf}) $$\n\nModel-free agents focus on learning values or policies directly from experience, without trying to model the environment's dynamics.\n\nAnother important distinction is between value-based and policy-based methods:\n\n### 10.3 Value-Based Agents\n\nThese agents learn a value function (either V or Q) and derive their policy from it:\n\n$$ A_{value} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{value}, V: S \\rightarrow \\mathbb{R}) $$\nor\n$$ A_{value} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{value}, Q: S \\times A \\rightarrow \\mathbb{R}) $$\n\n### 10.4 Policy-Based Agents\n\nThese agents directly learn a policy without necessarily learning a value function:\n\n$$ A_{policy} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{policy}) $$\n\n### 10.5 Actor-Critic Agents\n\nThese combine aspects of both value-based and policy-based methods:\n\n$$ A_{ac} = (S, A, \\pi, \\gamma, \\pi_{actor}: S \\rightarrow A, V_{critic}: S \\rightarrow \\mathbb{R}, L_{ac}) $$\n\nActor-Critic methods maintain both a policy (the actor) and a value function (the critic), using the critic to update the actor.\n\nFinally, we can distinguish between on-policy and off-policy methods:\n\n### 10.6 On-Policy Agents\n\nThese agents learn about the policy they're currently following:\n\n$$ A_{on\\_policy} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{on\\_policy}) $$\n\n### 10.7 Off-Policy Agents\n\nThese agents can learn about a policy different from the one they're following:\n\n$$ A_{off\\_policy} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{off\\_policy}) $$\n\nUnderstanding these different types of agents and algorithms is crucial for choosing the right approach for a given problem. But how can we improve the performance of these agents?\n\n## 11. Improving Agent Performance\n\nThere are several strategies for enhancing agent performance:\n\n1. Better Learning Algorithms: For example, Double Q-learning to reduce overestimation bias:\n\n   $$ Q_1(s, a) \\leftarrow Q_1(s, a) + \\alpha [r + \\gamma Q_2(s', \\arg\\max_{a'} Q_1(s', a')) - Q_1(s, a)] $$\n   $$ Q_2(s, a) \\leftarrow Q_2(s, a) + \\alpha [r + \\gamma Q_1(s', \\arg\\max_{a'} Q_2(s', a')) - Q_2(s, a)] $$\n\n2. Experience Replay: Storing and reusing past experiences to improve sample efficiency.\n\n3. Prioritized Experience Replay: Sampling important experiences more frequently:\n\n   $$ P(i) = \\frac{|\\delta_i| + \\epsilon}{\\sum_k (|\\delta_k| + \\epsilon)} $$\n\n   Where $P(i)$ is the probability of sampling experience $i$, $\\delta_i$ is the TD error of experience $i$, and $\\epsilon$ is a small constant.\n\n4. Function Approximation: Using neural networks to handle large state spaces.\n\n5. Exploration Strategies: Balancing exploration and exploitation to discover optimal policies.\n\nThese techniques help address various challenges in RL, such as sample efficiency, stability of learning, and scalability to complex environments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn agent in Reinforcement Learning (RL) is something that\ninteracts with [[Environment]]\nmakes decisions to maximize cumulative rewards. \n\nYou can think of it as a tuple:\n\n\\[ A = (S, A, \\pi, \\gamma) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n\nQuestion: What is an agent's state space in RL?\nEquation:\n\\[ A = (S, A, \\pi, \\gamma) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n\nQuestion: What is an agent's action space in RL?\nEquation:\n\\[ A = (S, A, \\pi, \\gamma) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n\nQuestion: What is an agent's policy in RL?\nEquation:\n\\[ A = (S, A, \\pi: S \\rightarrow A, \\gamma) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n\nFor deterministic policies:\n\\[ a_t = \\mu(s_t) \\]\n\nFor stochastic policies:\n\\[ a_t \\sim \\pi(\\cdot | s_t) \\]\n\nQuestion: What does an agent's learning algorithm do in RL?\nEquation:\n\\[ A = (S, A, \\pi: S \\rightarrow A, \\gamma, L) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( L \\): Learning Algorithm\n\nFor example, in Q-learning, the update rule is:\n\\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(s, a)] \\]\nwhere:\n- \\(Q(s, a)\\) is the action-value function.\n- \\(\\alpha\\) is the learning rate.\n- \\(R_{t+1}\\) is the reward received after taking action \\(a\\) in state \\(s\\).\n- \\(\\gamma\\) is the discount factor.\n- \\(S_{t+1}\\) is the next state.\n- \\(a'\\) is the next action.\n\nQuestion: What is an agent's reward function in RL?\nEquation:\n\\[ A = (S, A, R: S \\times A \\times S \\rightarrow \\mathbb{R}, \\pi: S \\rightarrow A, \\gamma) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( R: S \\times A \\times S \\rightarrow \\mathbb{R} \\): Reward Function\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n\nThe reward function can be defined as:\n\\[ r_t = R(s_t, a_t, s_{t+1}) \\]\n\nQuestion: What is a state-value based agent in RL?\nEquation:\n\\[ A_{state\\_value} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{sv}, V: S \\rightarrow \\mathbb{R}) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( L_{sv} \\): State-Value Learning Algorithm\n- \\( V: S \\rightarrow \\mathbb{R} \\): State-Value Function (learned)\n\nThe state-value function \\(V(s)\\) can be formally defined as:\n\\[ V(s) = \\mathbb{E}[G_t | S_t = s] = \\mathbb{E} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Bigg| S_t = s \\right] \\]\n\nQuestion: What is an action-value based agent in RL?\nEquation:\n\\[ A_{action\\_value} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{av}, Q: S \\times A \\rightarrow \\mathbb{R}) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( L_{av} \\): Action-Value Learning Algorithm\n- \\( Q: S \\times A \\rightarrow \\mathbb{R} \\): Action-Value Function (learned)\n\nThe action-value function \\(Q(s, a)\\) can be formally defined as:\n\\[ Q(s, a) = \\mathbb{E}[G_t | S_t = s, A_t = a] = \\mathbb{E} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Bigg| S_t = s, A_t = a \\right] \\]\n\nQuestion: What is a memory-based agent in RL?\nEquation:\n\\[ A_{memory} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{memory}, M) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( L_{memory} \\): Memory-Based Learning Algorithm\n- \\( M \\): Memory Mechanism (e.g., experience replay, recurrent neural network)\n\nThe memory mechanism \\(M\\) can be used in various ways, such as experience replay in DQN, where the agent stores and samples past experiences:\n\\[ M = \\{(s_t, a_t, r_{t+1}, s_{t+1})\\} \\]\n\nQuestion: What is a model-based agent in RL?\nEquation:\n\\[ A_{mb} = (S, A, \\hat{R}, \\hat{P}, \\pi: S \\rightarrow A, \\gamma, L_{mb}, PA) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\hat{R}: S \\times A \\times S \\rightarrow \\mathbb{R} \\): Estimated Reward Function (learned)\n- \\( \\hat{P}: S \\times A \\times S \\rightarrow [0, 1] \\): Estimated Transition Probability Function (learned)\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( L_{mb} \\): Model-Based Learning Algorithm\n- \\( PA \\): Planning Algorithm\n\nThe model of the environment consists of the transition function \\(\\hat{P}\\) and the reward function \\(\\hat{R}\\):\n\\[ \\hat{P}(s'|s, a) \\approx P(s'|s, a) \\]\n\\[ \\hat{R}(s, a, s') \\approx R(s, a, s') \\]\n\nQuestion:\n\n What is a model-free agent in RL?\nEquation:\nFor a model-free agent using Q-learning:\n\\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(s, a)] \\]\nwhere:\n- \\(Q(s, a)\\) is the action-value function.\n- \\(\\alpha\\) is the learning rate.\n- \\(R_{t+1}\\) is the reward received after taking action \\(a\\) in state \\(s\\).\n- \\(\\gamma\\) is the discount factor.\n- \\(S_{t+1}\\) is the next state.\n- \\(a'\\) is the next action.\n\nQuestion: What is an on-policy agent in RL?\nEquation:\n\\[ A_{on\\_policy} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{on\\_policy}) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( L_{on\\_policy} \\): On-Policy Learning Algorithm\n\nFor example, in the SARSA (State-Action-Reward-State-Action) algorithm:\n\\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(s, a)] \\]\nwhere:\n- \\(Q(s, a)\\) is the action-value function.\n- \\(\\alpha\\) is the learning rate.\n- \\(R_{t+1}\\) is the reward received after taking action \\(a\\) in state \\(s\\).\n- \\(\\gamma\\) is the discount factor.\n- \\(S_{t+1}\\) is the next state.\n- \\(A_{t+1}\\) is the next action taken according to the current policy \\(\\pi\\).\n\nQuestion: What is an off-policy agent in RL?\nEquation:\n\\[ A_{off\\_policy} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{off\\_policy}) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( L_{off\\_policy} \\): Off-Policy Learning Algorithm\n\nFor example, in Q-learning:\n\\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(s, a)] \\]\nwhere:\n- \\(Q(s, a)\\) is the action-value function.\n- \\(\\alpha\\) is the learning rate.\n- \\(R_{t+1}\\) is the reward received after taking action \\(a\\) in state \\(s\\).\n- \\(\\gamma\\) is the discount factor.\n- \\(S_{t+1}\\) is the next state.\n- \\(a'\\) is the action that maximizes \\(Q\\) in the next state \\(S_{t+1}\\).\n\nQuestion: What is an actor-critic agent in RL?\nEquation:\n\\[ A_{ac} = (S, A, \\pi, \\gamma, \\pi_{actor}, V_{critic}, L_{ac}) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( \\pi_{actor}: S \\rightarrow A\\): Actor (Policy Network) (learned)\n- \\( V_{critic}: S \\rightarrow \\mathbb{R}\\): Critic (Value Function Network) (learned)\n- \\( L_{ac}\\): Actor-Critic Learning Algorithm\n\nIn the Actor-Critic method, the update rules for the actor and critic are:\n\nCritic Update (TD Error):\n\\[ \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\]\n\nActor Update (Policy Gradient):\n\\[ \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_{\\theta} \\log \\pi(a|s, \\theta) \\]\nwhere:\n- \\(\\delta_t\\) is the temporal difference (TD) error.\n- \\(V(s)\\) is the state-value function.\n- \\(\\alpha\\) is the learning rate.\n- \\(\\theta\\) are the parameters of the policy \\(\\pi\\).\n- \\(\\pi(a|s, \\theta)\\) is the probability of taking action \\(a\\) in state \\(s\\) under policy parameters \\(\\theta\\).\n\nQuestion: What is an online learning agent in RL?\nEquation:\n\\[ A_{online} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{online}) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( L_{online} \\): Online Learning Algorithm\n\nFor example, in online Q-learning, the update occurs after each action:\n\\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)] \\]\nwhere \\(r\\) is the reward received.\n\nQuestion: What is an offline learning agent in RL?\nEquation:\n\\[ A_{offline} = (S, A, \\pi: S \\rightarrow A, \\gamma, L_{offline}, M) \\]\n- \\( S = \\{s_1, s_2, s_3, \\ldots, s_n\\} \\): State Space\n- \\( A = \\{a_1, a_2, a_3, \\ldots, a_m\\} \\): Action Space\n- \\( \\pi: S \\rightarrow A \\) : Policy (learned)\n- \\( \\gamma \\): Discount Factor\n- \\( L_{offline} \\): Offline Learning Algorithm\n- \\( M \\): Memory Mechanism for storing experiences\n\nFor example, in offline Q-learning with experience replay, the update occurs using a batch of experiences:\n\\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)] \\]\nwhere the experiences \\((s, a, r, s')\\) are sampled from memory \\(M\\).\n\nQuestion: How do we measure the performance of agents in RL?\nEquation:\n\\[ \\text{Performance}(A) = \\mathbb{E} \\left[ \\sum_{t=0}^{T} \\gamma^t R_t \\right] \\]\n\nEquation:\n- \\( G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\): Expected cumulative reward\n- \\( \\gamma \\): Discount Factor\n\nQuestion: How do we measure agent performance in RL (metrics)?\nEquation:\n\\[ G_t = \\sum_{t=0}^{T} R_t \\]\nwhere \\(T\\) is the total time horizon.\n\nQuestion: How do we improve the performance of agents in RL?\nEquation:\nExample of enhancing a learning algorithm (Double Q-learning):\n\\[ Q_1(s, a) \\leftarrow Q_1(s, a) + \\alpha [r + \\gamma Q_2(s', \\arg\\max_{a'} Q_1(s', a')) - Q_1(s, a)] \\]\n\\[ Q_2(s, a) \\leftarrow Q_2(s, a) + \\alpha [r + \\gamma Q_1(s', \\arg\\max_{a'} Q_2(s', a')) - Q_2(s, a)] \\]\n\nQuestion: How do we improve the learning efficiency of RL agents?\nEquation:\nExample of prioritized experience replay:\n\\[ P(i) = \\frac{|\\delta_i| + \\epsilon}{\\sum_k (|\\delta_k| + \\epsilon)} \\]\nwhere \\(P(i)\\) is the probability of sampling experience \\(i\\), \\(\\delta_i\\) is the TD error of experience \\(i\\), and \\(\\epsilon\\) is a small constant to ensure all experiences have a non-zero probability of being sampled.\n\n"}}]);
//# sourceMappingURL=579.6fcb7568.js.map