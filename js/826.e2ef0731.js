"use strict";(self["webpackChunkpersonal_website"]=self["webpackChunkpersonal_website"]||[]).push([[826],{2826:function(e,t,n){n.r(t),t["default"]="https://arxiv.org/abs/1706.03762\n\n## Stuff\n\n### 1. Input Embeddings and Position Encoding\n\n**Input Embeddings:**\n\n\\[ \\mathbf{X} \\in \\mathbb{R}^{L \\times d} \\]\n- **\\(\\mathbf{X}\\)**: Input embedding matrix where \\(L\\) is the sequence length and \\(d\\) is the model dimension.\n\n**Position Encoding:**\n\\[\n\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right), \\quad \\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n\\]\n- **\\(\\text{PE}_{(pos, 2i)}\\) and \\(\\text{PE}_{(pos, 2i+1)}\\)**: Position encoding values added to input embeddings to provide positional information.\n\n**Input with Position Encoding:**\n\\[ \\mathbf{X}_{\\text{pos}} = \\mathbf{X} + \\mathbf{PE} \\]\n- **\\(\\mathbf{X}_{\\text{pos}}\\)**: Input embeddings with position encoding added.\n\n### 2. Compute Query, Key, and Value Vectors\n\n\\[\n\\mathbf{Q} = \\mathbf{X}_{\\text{pos}} \\mathbf{W}^q, \\quad \\mathbf{K} = \\mathbf{X}_{\\text{pos}} \\mathbf{W}^k, \\quad \\mathbf{V} = \\mathbf{X}_{\\text{pos}} \\mathbf{W}^v\n\\]\n- **\\(\\mathbf{Q}\\)**: Query matrix.\n- **\\(\\mathbf{K}\\)**: Key matrix.\n- **\\(\\mathbf{V}\\)**: Value matrix.\n- **\\(\\mathbf{W}^q, \\mathbf{W}^k, \\mathbf{W}^v \\in \\mathbb{R}^{d \\times d_k}\\)**: Weight matrices for query, key, and value transformations.\n\n### 3. Scaled Dot-Product Attention\n\nFor each head \\( i \\):\n\n\\[\n\\mathbf{Q}_i = \\mathbf{Q} \\mathbf{W}^q_i, \\quad \\mathbf{K}_i = \\mathbf{K} \\mathbf{W}^k_i, \\quad \\mathbf{V}_i = \\mathbf{V} \\mathbf{W}^v_i\n\\]\n- **\\(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i\\)**: Projections of query, key, and value matrices for head \\(i\\).\n- **\\(\\mathbf{W}^q_i, \\mathbf{W}^k_i, \\mathbf{W}^v_i \\in \\mathbb{R}^{d \\times d_k}\\)**: Weight matrices for each head.\n\n**Attention Calculation:**\n\\[\n\\text{Attention}(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i) = \\text{softmax} \\left( \\frac{\\mathbf{Q}_i \\mathbf{K}_i^\\top}{\\sqrt{d_k}} \\right) \\mathbf{V}_i\n\\]\n- **\\(\\text{Attention}(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i)\\)**: Attention output for head \\(i\\).\n- **\\(\\text{softmax} \\left( \\frac{\\mathbf{Q}_i \\mathbf{K}_i^\\top}{\\sqrt{d_k}} \\right)\\)**: Scaled dot-product of query and key matrices followed by softmax to obtain attention weights.\n\n### 4. Multi-Head Attention\n\n**Concatenate the outputs of all heads:**\n\\[\n\\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\in \\mathbb{R}^{L \\times (h \\cdot d_k)}\n\\]\n- **\\(\\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)\\)**: Concatenation of outputs from all heads.\n\nwhere\n\\[\n\\text{head}_i = \\text{Attention}(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i)\n\\]\n- **\\(\\text{head}_i\\)**: Output from attention mechanism for head \\(i\\).\n\n**Final Linear Transformation:**\n\\[\n\\text{MultiHeadAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\mathbf{W}^o\n\\]\n- **\\(\\mathbf{W}^o \\in \\mathbb{R}^{(h \\cdot d_k) \\times d}\\)**: Weight matrix for the final linear transformation after concatenation.\n\n### 5. Add & Norm\n\n\\[\n\\text{Add\\&Norm}(x, \\text{Sublayer}(x)) = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n\\]\n- **\\(\\text{Add\\&Norm}(x, \\text{Sublayer}(x))\\)**: Residual connection followed by layer normalization.\n- **\\(\\text{LayerNorm}(x + \\text{Sublayer}(x))\\)**: Layer normalization applied to the sum of input and sublayer output.\n\n### 6. Feed-Forward Network\n\n\\[\n\\text{FFN}(x) = \\max(0, x \\mathbf{W}_1 + b_1) \\mathbf{W}_2 + b_2\n\\]\n- **\\(\\text{FFN}(x)\\)**: Feed-forward network applied to each position separately.\n- **\\(\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times d_{\\text{ff}}}, \\mathbf{W}_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d}\\)**: Weight matrices for the feed-forward network.\n- **\\(b_1, b_2\\)**: Bias terms for the feed-forward network.\n\n### 7. Encoder Layer\n\n\\[\n\\text{EncoderLayer}(x) = \\text{Add\\&Norm}(x, \\text{MultiHeadAttention}(x, x, x)) \\rightarrow \\text{Add\\&Norm}(x, \\text{FFN}(x))\n\\]\n- **\\(\\text{EncoderLayer}(x)\\)**: Single encoder layer consisting of multi-head attention and feed-forward network with add & norm.\n\n### 8. Decoder Layer\n\n\\[\n\\text{DecoderLayer}(y, \\text{encoder\\_output}) = \\text{Add\\&Norm}(y, \\text{MaskedMultiHeadAttention}(y, y, y)) \\rightarrow \\text{Add\\&Norm}(y, \\text{MultiHeadAttention}(y, \\text{encoder\\_output}, \\text{encoder\\_output})) \\rightarrow \\text{Add\\&Norm}(y, \\text{FFN}(y))\n\\]\n- **\\(\\text{DecoderLayer}(y, \\text{encoder\\_output})\\)**: Single decoder layer with masked multi-head attention, encoder-decoder attention, and feed-forward network with add & norm.\n- **\\(\\text{MaskedMultiHeadAttention}(y, y, y)\\)**: Masked multi-head attention to prevent positions from attending to subsequent positions.\n- **\\(\\text{MultiHeadAttention}(y, \\text{encoder\\_output}, \\text{encoder\\_output})\\)**: Multi-head attention using encoder's output.\n\n### 9. Masked Multi-Head Attention\n\n\\[\n\\text{MaskedAttention}(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i) = \\text{softmax} \\left( \\frac{\\mathbf{Q}_i \\mathbf{K}_i^\\top}{\\sqrt{d_k}} + \\mathbf{M} \\right) \\mathbf{V}_i\n\\]\n- **\\(\\text{MaskedAttention}(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i)\\)**: Masked attention mechanism to ensure causality in the decoder.\n- **\\(\\mathbf{M}\\)**: Mask matrix to prevent attention to future positions.\n\n### 10. Final Linear and Softmax Layer\n\n\\[\n\\text{OutputProbs} = \\text{softmax}(\\text{Linear}(\\text{DecoderOutput}))\n\\]\n- **\\(\\text{OutputProbs}\\)**: Final output probabilities after applying a linear transformation and softmax.\n- **\\(\\text{Linear}(\\text{DecoderOutput})\\)**: Linear transformation applied to the decoder output.\n\n### Summary\n\n1. **Input Embeddings and Position Encoding:**\n   \\[\n   \\mathbf{X}_{\\text{pos}} = \\mathbf{X} + \\mathbf{PE}\n   \\]\n\n2. **Compute Query, Key, and Value Vectors:**\n   \\[\n   \\mathbf{Q} = \\mathbf{X}_{\\text{pos}} \\mathbf{W}^q, \\quad \\mathbf{K} = \\mathbf{X}_{\\text{pos}} \\mathbf{W}^k, \\quad \\mathbf{V} = \\mathbf{X}_{\\text{pos}} \\mathbf{W}^v\n   \\]\n\n3\n\n. **Scaled Dot-Product Attention:**\n   \\[\n   \\text{Attention}(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i) = \\text{softmax} \\left( \\frac{\\mathbf{Q}_i \\mathbf{K}_i^\\top}{\\sqrt{d_k}} \\right) \\mathbf{V}_i\n   \\]\n\n4. **Multi-Head Attention:**\n   \\[\n   \\text{MultiHeadAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\mathbf{W}^o\n   \\]\n\n5. **Add & Norm:**\n   \\[\n   \\text{Add\\&Norm}(x, \\text{Sublayer}(x)) = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n   \\]\n\n6. **Feed-Forward Network:**\n   \\[\n   \\text{FFN}(x) = \\max(0, x \\mathbf{W}_1 + b_1) \\mathbf{W}_2 + b_2\n   \\]\n\n7. **Encoder Layer:**\n   \\[\n   \\text{EncoderLayer}(x) = \\text{Add\\&Norm}(x, \\text{MultiHeadAttention}(x, x, x)) \\rightarrow \\text{Add\\&Norm}(x, \\text{FFN}(x))\n   \\]\n\n8. **Decoder Layer:**\n   \\[\n   \\text{DecoderLayer}(y, \\text{encoder\\_output}) = \\text{Add\\&Norm}(y, \\text{MaskedMultiHeadAttention}(y, y, y)) \\rightarrow \\text{Add\\&Norm}(y, \\text{MultiHeadAttention}(y, \\text{encoder\\_output}, \\text{encoder\\_output})) \\rightarrow \\text{Add\\&Norm}(y, \\text{FFN}(y))\n   \\]\n\n9. **Masked Multi-Head Attention:**\n   \\[\n   \\text{MaskedAttention}(\\mathbf{Q}_i, \\mathbf{K}_i, \\mathbf{V}_i) = \\text{softmax} \\left( \\frac{\\mathbf{Q}_i \\mathbf{K}_i^\\top}{\\sqrt{d_k}} + \\mathbf{M} \\right) \\mathbf{V}_i\n   \\]\n\n10. **Final Linear and Softmax Layer:**\n    \\[\n    \\text{OutputProbs} = \\text{softmax}(\\text{Linear}(\\text{DecoderOutput}))\n    \\]\n\nThis detailed breakdown with explanations should provide a comprehensive understanding of the transformer's mechanisms from end to end.\n\nQuestion;Answer;UUID\nWhat is the general purpose of attention mechanisms?;The general purpose of attention mechanisms is to allow the model to focus dynamically on different parts of the input sequence when processing each token. This helps the model to selectively concentrate on relevant information, improving its ability to understand context and relationships within the data.;faac118b-14d2-4225-82ce-5a1c2b22f894\nWhat is the difference between attention and self-attention?;- Attention: Refers to mechanisms where the model focuses on specific parts of the input sequence, often used in encoder-decoder models (e.g., for translation, where the decoder attends to the encoder's output).<br> - Self-Attention: A specific type of attention where each token in the input sequence attends to all other tokens within the same sequence. It helps to capture dependencies and relationships within the sequence itself.;ae4c7a16-d2e2-4f31-9acd-dbd039b64835\nHow does the information given by attention differ from what we get from embeddings?;- Embeddings: Provide static representations of tokens based on their overall usage in the training corpus. <br> - Attention: Dynamically adjusts these embeddings based on the specific context of the sequence, allowing the model to focus on relevant parts of the input at each step.;175e7776-aec6-4d3b-9c99-a5ddc61b4d1b\nWhat is the query vector?;The query vector (\\(\\mathbf{Q}\\)) represents the current token's perspective or the \"question\" it is asking about the sequence. It is derived from the input embeddings through a linear transformation.;d33e3f45-53d6-4d4d-860c-b337569cd772\nWhat is the key vector?;The key vector (\\(\\mathbf{K}\\)) represents the \"identifiers\" for each token in the sequence. It is also derived from the input embeddings through a separate linear transformation.;f6d632ce-926c-455c-afde-01f5b0b41da9\nWhat is the value vector?;The value vector (\\(\\mathbf{V}\\)) represents the actual content or information of each token. It is derived from the input embeddings through yet another linear transformation.;c7eeb5ed-f28b-4936-b249-b8c332f2147c\nWhy are the query, key, value matrices necessary in attention?;The query, key, and value matrices (\\(\\mathbf{W}^q\\), \\(\\mathbf{W}^k\\), \\(\\mathbf{W}^v\\)) are necessary because they transform the static embeddings into representations that can dynamically interact with each other to compute attention scores and context-aware values. These are learned linear transformations that dynamically adjust the embeddings to focus on context-specific relationships within a sequence.;f8c4ae7a-f0a2-496c-a1da-409a89365eaf\nWhat parts of the attention mechanism are learned?;The weight matrices \\(\\mathbf{W}^q\\), \\(\\mathbf{W}^k\\), \\(\\mathbf{W}^v\\), and the output projection matrix \\(\\mathbf{W}^o\\) are all learned during training. These matrices are adjusted to minimize the loss function and improve the model's performance on the given task.;41dba38c-b41f-49bb-81a9-be9d183c5ff1\nWhy are learned representations helpful for attention?;Learned representations allow the model to adapt to the specific context and task, capturing intricate relationships and dependencies within the data that static representations might miss.;d85c3f37-007b-471c-b082-790df1a395a3\nWhat intuitively are the query, key, value weight matrices learning?;- Query Matrix (\\(\\mathbf{W}^q\\)): Learns to transform the embedding of a token to reflect what it is \"interested in\" or what information it is seeking in a specific context. <br> - Key Matrix (\\(\\mathbf{W}^k\\)): Learns to transform each token's embedding to highlight its identity and role in a way that can be recognized by queries from other tokens. <br> - Value Matrix (\\(\\mathbf{W}^v\\)): Learns to transform the embeddings to carry useful contextual information that will be aggregated based on the attention scores. <br>;da233721-d209-4d60-ad10-7b674ff5133c\nWhat intuitively is the multi-head projection weight matrix learning?;The multi-head projection weight matrix (\\(\\mathbf{W}^o\\)) learns to combine the outputs of multiple attention heads into a single cohesive representation. Each head might capture different aspects of the relationships, and \\(\\mathbf{W}^o\\) integrates these diverse perspectives into a unified representation. <br>;bdde49f8-e02d-451a-985e-451b997add1d\nWhat intuitively is the final fully connected network learning?;The final fully connected network (Feed-Forward Network, FFN) learns to further process the aggregated attention outputs, capturing complex patterns and dependencies that enhance the model's understanding and representation of the sequence. <br>;8b610200-c839-437c-b686-587c4d0e4e84\nWhat is the input to the attention mechanism?;The input is the sequence of token embeddings with positional encodings added: \\[ \\mathbf{X}_{\\text{pos}} = \\mathbf{X} + \\mathbf{PE} \\];87bd25b1-c7ad-4980-ad26-87ade739ac9b\nWhat do we do after including positional information to calculate self-attention?;We transform the input embeddings with positional information into query, key, and value vectors using the learned weight matrices: \\[ \\mathbf{Q} = \\mathbf{X}_{\\text{pos}} \\mathbf{W}^q, \\quad \\mathbf{K} = \\mathbf{X}_{\\text{pos}} \\mathbf{W}^k, \\quad \\mathbf{V} = \\mathbf{X}_{\\text{pos}} \\mathbf{W}^v \\];3c0dec69-9092-4578-9c1f-847e1ec7d570\nWhat do we do after cloning the input vector to calculate self-attention?;The term \"cloning\" here can be understood as passing the same input vector to multiple transformations (for Q, K, V) and multiple heads in multi-head attention.;8f0476b1-1619-4ca5-9d8f-aff78702b1c2\nWhat do we do after creating query, key, and value vectors to calculate self-attention?;We calculate the dot product of the query and key vectors to get attention scores: \\[ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}} \\right) \\mathbf{V} \\];c8c4a8ac-60fb-4b9e-ac6a-694c529fb94e\nWhat do we do after the dot product to calculate self-attention?;We scale the dot product by the square root of the dimension of the key vectors to stabilize gradients: \\[ \\text{Scaled Scores} = \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}} \\];83e2b643-7d18-4860-b895-572ba29be5f1\nWhat do we do after scaling to calculate self-attention?;We apply the softmax function to the scaled scores to get attention weights: \\[ \\text{Attention Weights} = \\text{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}} \\right) \\];3f56c0ef-e4b8-4ad3-a483-8372a778c8ad\nWhat do we do after softmax to calculate self-attention?;We use the attention weights to compute a weighted sum of the value vectors: \\[ \\text{Attention Output} = \\text{Attention Weights} \\cdot \\mathbf{V} \\];7112cf21-7d69-4f6d-a09d-fa1f5a888812\nWhat do we do after getting attention scores to calculate self-attention?;After obtaining the attention output, we apply a residual connection and layer normalization: \\[ \\text{Add\\&Norm}(x, \\text{Attention Output}) = \\text{LayerNorm}(x + \\text{Attention Output}) \\];f6a0f43d-5fb5-43ed-9358-e55818eb6177\nWhat do we do after the residual connection to calculate self-attention?;We pass the normalized output through a feed-forward network (FFN) and apply another residual connection and layer normalization: \\[ \\text{Add\\&Norm}(x, \\text{FFN Output}) = \\text{LayerNorm}(x + \\text{FFN Output}) \\];a6c62f11-6f07-451d-ad75-5496da15b0a4\nWhy do we need to include positional information to calculate self-attention?;Positional information is needed because self-attention mechanisms do not inherently capture the order of tokens. Adding positional encodings allows the model to understand the sequence order.;c3f040f3-26ee-4e54-95d3-6471da450fa9\nWhy do we need to clone the input vector to calculate self-attention?;Cloning or passing the same input to multiple transformations ensures that each token can simultaneously serve as a query, key, and value, enabling the model to capture relationships within the sequence.;5969630a-8694-4938-8484-7611921e4431\nWhy do we need to create query, key, and value vectors to calculate self-attention?;Creating query, key, and value vectors allows the model to dynamically compute attention scores and contextually relevant representations based on the specific requirements of the task.;bceb47e5-cb9d-49af-b8fa-6e12a713a1af\nWhy do we need the dot product to calculate self-attention?;The dot product measures the similarity between the query and key vectors, which helps determine how much attention each token should pay to every other token.;4d0f0bc3-4435-4b11-b6cb-02bc24bd20d4\nWhy do we need to scale/normalize to calculate self-attention?;Scaling by \\(\\sqrt{d_k}\\) prevents the dot product values from becoming too large, which can destabilize gradients and slow down training.;8ad94581-5c54-4d7c-ac43-7b62b2a470b3\nWhy do we need to use softmax to calculate self-attention?;Softmax normalizes the attention scores into probabilities, ensuring that the weights sum to 1 and that each token's contribution is appropriately weighted.;ae480e38-fb59-4f88-ad8c-1b9d0558abde\nWhy do we need to use a residual connection to calculate self-attention?;Residual connections help prevent vanishing gradients and allow the model to retain useful information from the original input, improving training stability and performance.;53eb139a-aafb-47ca-a704-8d4ab3112c09\nWhat do we do after concatenating attention heads to calculate multi-headed self-attention?;We apply a final linear transformation using the output projection matrix: \\[ \\ text{MultiHeadAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\mathbf{W}^o \\];322ee9c3-6479-4e57-9567-e27ea2010865\nWhat do we do after multiplying by the projection matrix to calculate multi-headed self-attention?;We pass the projected output through a residual connection and layer normalization: \\[ \\text{Add\\&Norm}(x, \\text{MultiHeadAttention Output}) = \\text{LayerNorm}(x + \\text{MultiHeadAttention Output}) \\];bd7a5955-63b8-4eb3-b9b5-469fd6f0c558\nWhat do we do after the fully connected network to calculate multi-headed self-attention?;We apply another residual connection and layer normalization to the output of the feed-forward network: \\[ \\text{Add\\&Norm}(x, \\text{FFN Output}) = \\text{LayerNorm}(x + \\text{FFN Output}) \\];b9c2ef70-fd0f-42f3-877e-b037f0919530\nWhy do we need to use multiple heads to calculate self-attention?;Multiple heads allow the model to capture different types of relationships and dependencies within the sequence, providing a richer and more nuanced representation.;4cc5e9db-f532-4ab7-b105-f8cf841a379c\nWhy do we need to concatenate attention to calculate multi-headed self-attention?;Concatenation combines the outputs of all attention heads, integrating the diverse information captured by each head into a unified representation.;98ebb91e-1e43-4735-9675-62eef597c1d9\nWhy do we need to multiply by a projection matrix to calculate multi-headed self-attention?;Multiplying by the projection matrix (\\(\\mathbf{W}^o\\)) transforms the concatenated outputs into the appropriate dimension for further processing in the model.;32658048-0d46-4faf-85a8-f8c2f2a494ed\nWhy do we need the fully connected network to calculate multi-headed self-attention?;The fully connected network (FFN) processes the attention outputs to capture complex patterns and dependencies, enhancing the model's ability to understand and represent the input sequence.;42e3d822-b2c8-4558-89d4-bf4f67593806\nHow does self-attention provide contextualized representations?;Self-attention aggregates information from the entire sequence, allowing each token to be represented in the context of its surroundings, resulting in contextually-aware embeddings.;69e07278-afb5-4c49-8c1c-631ac8c72306\nHow does self-attention enable focusing on relevant information?;Self-attention computes attention scores that determine the relevance of each token to others in the sequence, allowing the model to focus on the most important parts of the input.;db9f65a2-ae81-4098-bf62-119bebb7b2d4\nHow does self-attention enable efficient relationship modeling?;Self-attention allows the model to capture relationships between tokens regardless of their distance in the sequence, efficiently modeling both local and global dependencies.;5a9b1f6a-dead-4f23-afba-91f48aacdf70\nHow does self-attention enable discovering long-range dependencies?;By attending to all tokens in the sequence, self-attention can capture long-range dependencies that are crucial for understanding the context and meaning of the input.;ec524dc4-c796-46c3-984c-f4e42ac2cd4f\nHow does attention enable parallel processing?;Attention mechanisms, especially self-attention, allow for parallel processing of tokens, as each token independently attends to all others, improving computational efficiency.;ea0d1e7d-84d6-4c50-b58b-308d5730b52a\nWhat is the computational complexity of self-attention?;The computational complexity of self-attention is \\(O(L^2 \\cdot d)\\), where \\(L\\) is the sequence length and \\(d\\) is the model dimension. This is due to the pairwise attention score calculations.;0cd84c0a-2434-41c5-941f-d3e47a46f00e\nWhy is dynamic contextualization important in self-attention?;Embeddings: Provide a fixed representation of each token based on its overall usage in the training corpus.<br> Q, K, V Matrices: Allow the model to adapt the embeddings to the specific context of the sequence in which the token appears. This dynamic adjustment is crucial because the meaning of a word can change depending on its context.;87863c23-2111-4109-b7aa-c6e04aff3da0\nWhy is focusing on relevant information important in self-attention?;Embeddings: Contain general information about the token.<br> Q, K, V Matrices: Learn to highlight relevant features of the token for the task at hand. For example, the key vectors might emphasize syntactic roles, while the query vectors focus on the semantic relationships needed to answer specific queries.;0fb8c4a1-a717-492f-8341-f41a74d5fe86\nWhy is efficient relationship modeling important in self-attention?;Embeddings: Represent tokens independently.<br> Q, K, V Matrices: Enable the model to compute relationships between tokens efficiently. By transforming embeddings into Q and K vectors, the dot product operation (Q⋅K) effectively measures the relevance or attention score between tokens.;72db265c-41e3-4ecf-be84-e6696a339ddc\n\nQuestion;Answer;UUID\nWhat is the structure of an encoder?;The encoder typically consists of multiple identical layers, each containing two main sub-layers: a multi-head self-attention mechanism and a position-wise feed-forward network. Each sub-layer is followed by a residual connection and layer normalization.;ba60f6db-f9da-48f3-abbb-163125eb7e53\nWhat is the purpose of the encoder?;The purpose of the encoder is to process the input sequence and create a rich, contextualized representation of each token. This representation captures the relationships and dependencies within the input, which can then be used by the decoder or for other downstream tasks.;e5eec82c-6d33-44ed-b2c1-0497bb54190a\nHow does the encoder process input data?;The encoder processes input data through these steps: Convert input tokens to embeddings and add positional encodings Pass through multiple encoder layers, each containing: a. Multi-head self-attention mechanism b. Feed-forward network c. Residual connections and layer normalization after each sub-layer Output the final contextualized representations;fe6eea66-c21c-4c5d-b7b5-20fdebe4a0fa\nWhat is the role of self-attention in the encoder?;Self-attention in the encoder allows each token to attend to all other tokens in the input sequence, including itself. This enables the model to capture contextual relationships and dependencies between tokens, regardless of their position in the sequence.;38c1c8a2-083b-4f1b-a5b4-b1a1932101fe\nHow does the encoder's self-attention mechanism work?;The encoder's self-attention mechanism works as follows: Create query, key, and value vectors for each token Calculate attention scores by comparing each query with all keys Scale the scores and apply softmax to get attention weights Multiply the weights with value vectors to get the weighted sum Combine results from multiple attention heads Apply a final linear transformation;1f58da26-6dd7-43ca-be6d-5b105094e88a\nWhat is the feed-forward network in an encoder?;The feed-forward network in an encoder is a position-wise fully connected neural network applied to each position separately and identically. It typically consists of two linear transformations with a ReLU activation in between. Its purpose is to introduce non-linearity and further process the attention outputs.;486fcec3-d048-472c-9f18-ec25ff576961\nWhat happens after the self-attention mechanism in an encoder?;After the self-attention mechanism in an encoder: A residual connection is applied, adding the input to the attention output Layer normalization is performed on the combined result The normalized output is then passed to the feed-forward network;07e12560-0ea3-417f-9064-483ff009ebc4\nWhat is the role of layer normalization in the encoder?;Layer normalization in the encoder helps stabilize the learning process and reduces internal covariate shift. It normalizes the inputs across the features, allowing each layer to learn more independently of other layers and improving training speed and performance.;d84f4586-48df-4161-904e-a24a84bf9edd\nWhat do we do after applying self-attention in the encoder?;After applying self-attention in the encoder: Add a residual connection (add the input to the attention output) Apply layer normalization to the combined result Pass the normalized output to the feed-forward network;1e85830c-a08d-4cab-896a-eb216d592095\nWhat do we do after applying the feed-forward network in the encoder?;After applying the feed-forward network in the encoder: Add a residual connection (add the input to the feed-forward network output) Apply layer normalization to the combined result The output becomes the input for the next encoder layer or the final output if it's the last layer;9687ff6a-0e7b-4f67-b03b-78845ed90846\nHow does layer normalization differ from batch normalization?;Layer normalization differs from batch normalization in the following ways: Layer norm normalizes across features for each sample, while batch norm normalizes across the batch for each feature Layer norm is independent of batch size and works well with variable-length sequences Layer norm computes mean and variance for each sample, while batch norm uses batch statistics Layer norm is more commonly used in transformers and RNNs, while batch norm is often used in CNNs;392b5967-6688-4ace-ad73-f61d5bf6c2a0\nWhat are residual connections in the encoder and how are they implemented?;Residual connections in the encoder are skip connections that add the input of a sub-layer to its output. They are implemented by adding the input tensor to the output tensor of each sub-layer (self-attention and feed-forward network). The formula is: LayerNorm(x + Sublayer(x)), where x is the input and Sublayer(x) is the function implemented by the sub-layer itself.;991490e7-bbd4-4736-8f24-95e827769c9e\nWhy are residual connections important in the encoder?;Residual connections are important in the encoder for several reasons: They help mitigate the vanishing gradient problem in deep networks They allow the model to learn residual functions with reference to the layer inputs They enable the network to pass low-level features directly to higher layers They improve the flow of information and gradients throughout the network They make it easier for the network to learn identity mappings when needed They contribute to faster training and better performance of very deep networks;853bb95f-2c0e-446b-ab98-46298f3ea52f\n\n\n\n\n"}}]);
//# sourceMappingURL=826.e2ef0731.js.map