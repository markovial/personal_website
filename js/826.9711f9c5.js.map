{"version":3,"file":"js/826.9711f9c5.js","mappings":"sIAAA,6zP","sources":["webpack://personal_website/./src/assets/blog/transformers/transformers.md"],"sourcesContent":["export default \"Sure! Here is the step-by-step process with explanations for each variable:\\n\\n### 1. Input Embeddings and Position Encoding\\n\\n**Input Embeddings:**\\n\\\\[ \\\\mathbf{X} \\\\in \\\\mathbb{R}^{L \\\\times d} \\\\]\\n- **\\\\(\\\\mathbf{X}\\\\)**: Input embedding matrix where \\\\(L\\\\) is the sequence length and \\\\(d\\\\) is the model dimension.\\n\\n**Position Encoding:**\\n\\\\[\\n\\\\text{PE}_{(pos, 2i)} = \\\\sin\\\\left(\\\\frac{pos}{10000^{2i/d}}\\\\right), \\\\quad \\\\text{PE}_{(pos, 2i+1)} = \\\\cos\\\\left(\\\\frac{pos}{10000^{2i/d}}\\\\right)\\n\\\\]\\n- **\\\\(\\\\text{PE}_{(pos, 2i)}\\\\) and \\\\(\\\\text{PE}_{(pos, 2i+1)}\\\\)**: Position encoding values added to input embeddings to provide positional information.\\n\\n**Input with Position Encoding:**\\n\\\\[ \\\\mathbf{X}_{\\\\text{pos}} = \\\\mathbf{X} + \\\\mathbf{PE} \\\\]\\n- **\\\\(\\\\mathbf{X}_{\\\\text{pos}}\\\\)**: Input embeddings with position encoding added.\\n\\n### 2. Compute Query, Key, and Value Vectors\\n\\n\\\\[\\n\\\\mathbf{Q} = \\\\mathbf{X}_{\\\\text{pos}} \\\\mathbf{W}^q, \\\\quad \\\\mathbf{K} = \\\\mathbf{X}_{\\\\text{pos}} \\\\mathbf{W}^k, \\\\quad \\\\mathbf{V} = \\\\mathbf{X}_{\\\\text{pos}} \\\\mathbf{W}^v\\n\\\\]\\n- **\\\\(\\\\mathbf{Q}\\\\)**: Query matrix.\\n- **\\\\(\\\\mathbf{K}\\\\)**: Key matrix.\\n- **\\\\(\\\\mathbf{V}\\\\)**: Value matrix.\\n- **\\\\(\\\\mathbf{W}^q, \\\\mathbf{W}^k, \\\\mathbf{W}^v \\\\in \\\\mathbb{R}^{d \\\\times d_k}\\\\)**: Weight matrices for query, key, and value transformations.\\n\\n### 3. Scaled Dot-Product Attention\\n\\nFor each head \\\\( i \\\\):\\n\\n\\\\[\\n\\\\mathbf{Q}_i = \\\\mathbf{Q} \\\\mathbf{W}^q_i, \\\\quad \\\\mathbf{K}_i = \\\\mathbf{K} \\\\mathbf{W}^k_i, \\\\quad \\\\mathbf{V}_i = \\\\mathbf{V} \\\\mathbf{W}^v_i\\n\\\\]\\n- **\\\\(\\\\mathbf{Q}_i, \\\\mathbf{K}_i, \\\\mathbf{V}_i\\\\)**: Projections of query, key, and value matrices for head \\\\(i\\\\).\\n- **\\\\(\\\\mathbf{W}^q_i, \\\\mathbf{W}^k_i, \\\\mathbf{W}^v_i \\\\in \\\\mathbb{R}^{d \\\\times d_k}\\\\)**: Weight matrices for each head.\\n\\n**Attention Calculation:**\\n\\\\[\\n\\\\text{Attention}(\\\\mathbf{Q}_i, \\\\mathbf{K}_i, \\\\mathbf{V}_i) = \\\\text{softmax} \\\\left( \\\\frac{\\\\mathbf{Q}_i \\\\mathbf{K}_i^\\\\top}{\\\\sqrt{d_k}} \\\\right) \\\\mathbf{V}_i\\n\\\\]\\n- **\\\\(\\\\text{Attention}(\\\\mathbf{Q}_i, \\\\mathbf{K}_i, \\\\mathbf{V}_i)\\\\)**: Attention output for head \\\\(i\\\\).\\n- **\\\\(\\\\text{softmax} \\\\left( \\\\frac{\\\\mathbf{Q}_i \\\\mathbf{K}_i^\\\\top}{\\\\sqrt{d_k}} \\\\right)\\\\)**: Scaled dot-product of query and key matrices followed by softmax to obtain attention weights.\\n\\n### 4. Multi-Head Attention\\n\\n**Concatenate the outputs of all heads:**\\n\\\\[\\n\\\\text{Concat}(\\\\text{head}_1, \\\\dots, \\\\text{head}_h) \\\\in \\\\mathbb{R}^{L \\\\times (h \\\\cdot d_k)}\\n\\\\]\\n- **\\\\(\\\\text{Concat}(\\\\text{head}_1, \\\\dots, \\\\text{head}_h)\\\\)**: Concatenation of outputs from all heads.\\n\\nwhere\\n\\\\[\\n\\\\text{head}_i = \\\\text{Attention}(\\\\mathbf{Q}_i, \\\\mathbf{K}_i, \\\\mathbf{V}_i)\\n\\\\]\\n- **\\\\(\\\\text{head}_i\\\\)**: Output from attention mechanism for head \\\\(i\\\\).\\n\\n**Final Linear Transformation:**\\n\\\\[\\n\\\\text{MultiHeadAttention}(\\\\mathbf{Q}, \\\\mathbf{K}, \\\\mathbf{V}) = \\\\text{Concat}(\\\\text{head}_1, \\\\dots, \\\\text{head}_h) \\\\mathbf{W}^o\\n\\\\]\\n- **\\\\(\\\\mathbf{W}^o \\\\in \\\\mathbb{R}^{(h \\\\cdot d_k) \\\\times d}\\\\)**: Weight matrix for the final linear transformation after concatenation.\\n\\n### 5. Add & Norm\\n\\n\\\\[\\n\\\\text{Add\\\\&Norm}(x, \\\\text{Sublayer}(x)) = \\\\text{LayerNorm}(x + \\\\text{Sublayer}(x))\\n\\\\]\\n- **\\\\(\\\\text{Add\\\\&Norm}(x, \\\\text{Sublayer}(x))\\\\)**: Residual connection followed by layer normalization.\\n- **\\\\(\\\\text{LayerNorm}(x + \\\\text{Sublayer}(x))\\\\)**: Layer normalization applied to the sum of input and sublayer output.\\n\\n### 6. Feed-Forward Network\\n\\n\\\\[\\n\\\\text{FFN}(x) = \\\\max(0, x \\\\mathbf{W}_1 + b_1) \\\\mathbf{W}_2 + b_2\\n\\\\]\\n- **\\\\(\\\\text{FFN}(x)\\\\)**: Feed-forward network applied to each position separately.\\n- **\\\\(\\\\mathbf{W}_1 \\\\in \\\\mathbb{R}^{d \\\\times d_{\\\\text{ff}}}, \\\\mathbf{W}_2 \\\\in \\\\mathbb{R}^{d_{\\\\text{ff}} \\\\times d}\\\\)**: Weight matrices for the feed-forward network.\\n- **\\\\(b_1, b_2\\\\)**: Bias terms for the feed-forward network.\\n\\n### 7. Encoder Layer\\n\\n\\\\[\\n\\\\text{EncoderLayer}(x) = \\\\text{Add\\\\&Norm}(x, \\\\text{MultiHeadAttention}(x, x, x)) \\\\rightarrow \\\\text{Add\\\\&Norm}(x, \\\\text{FFN}(x))\\n\\\\]\\n- **\\\\(\\\\text{EncoderLayer}(x)\\\\)**: Single encoder layer consisting of multi-head attention and feed-forward network with add & norm.\\n\\n### 8. Decoder Layer\\n\\n\\\\[\\n\\\\text{DecoderLayer}(y, \\\\text{encoder\\\\_output}) = \\\\text{Add\\\\&Norm}(y, \\\\text{MaskedMultiHeadAttention}(y, y, y)) \\\\rightarrow \\\\text{Add\\\\&Norm}(y, \\\\text{MultiHeadAttention}(y, \\\\text{encoder\\\\_output}, \\\\text{encoder\\\\_output})) \\\\rightarrow \\\\text{Add\\\\&Norm}(y, \\\\text{FFN}(y))\\n\\\\]\\n- **\\\\(\\\\text{DecoderLayer}(y, \\\\text{encoder\\\\_output})\\\\)**: Single decoder layer with masked multi-head attention, encoder-decoder attention, and feed-forward network with add & norm.\\n- **\\\\(\\\\text{MaskedMultiHeadAttention}(y, y, y)\\\\)**: Masked multi-head attention to prevent positions from attending to subsequent positions.\\n- **\\\\(\\\\text{MultiHeadAttention}(y, \\\\text{encoder\\\\_output}, \\\\text{encoder\\\\_output})\\\\)**: Multi-head attention using encoder's output.\\n\\n### 9. Masked Multi-Head Attention\\n\\n\\\\[\\n\\\\text{MaskedAttention}(\\\\mathbf{Q}_i, \\\\mathbf{K}_i, \\\\mathbf{V}_i) = \\\\text{softmax} \\\\left( \\\\frac{\\\\mathbf{Q}_i \\\\mathbf{K}_i^\\\\top}{\\\\sqrt{d_k}} + \\\\mathbf{M} \\\\right) \\\\mathbf{V}_i\\n\\\\]\\n- **\\\\(\\\\text{MaskedAttention}(\\\\mathbf{Q}_i, \\\\mathbf{K}_i, \\\\mathbf{V}_i)\\\\)**: Masked attention mechanism to ensure causality in the decoder.\\n- **\\\\(\\\\mathbf{M}\\\\)**: Mask matrix to prevent attention to future positions.\\n\\n### 10. Final Linear and Softmax Layer\\n\\n\\\\[\\n\\\\text{OutputProbs} = \\\\text{softmax}(\\\\text{Linear}(\\\\text{DecoderOutput}))\\n\\\\]\\n- **\\\\(\\\\text{OutputProbs}\\\\)**: Final output probabilities after applying a linear transformation and softmax.\\n- **\\\\(\\\\text{Linear}(\\\\text{DecoderOutput})\\\\)**: Linear transformation applied to the decoder output.\\n\\n### Summary\\n\\n1. **Input Embeddings and Position Encoding:**\\n   \\\\[\\n   \\\\mathbf{X}_{\\\\text{pos}} = \\\\mathbf{X} + \\\\mathbf{PE}\\n   \\\\]\\n\\n2. **Compute Query, Key, and Value Vectors:**\\n   \\\\[\\n   \\\\mathbf{Q} = \\\\mathbf{X}_{\\\\text{pos}} \\\\mathbf{W}^q, \\\\quad \\\\mathbf{K} = \\\\mathbf{X}_{\\\\text{pos}} \\\\mathbf{W}^k, \\\\quad \\\\mathbf{V} = \\\\mathbf{X}_{\\\\text{pos}} \\\\mathbf{W}^v\\n   \\\\]\\n\\n3\\n\\n. **Scaled Dot-Product Attention:**\\n   \\\\[\\n   \\\\text{Attention}(\\\\mathbf{Q}_i, \\\\mathbf{K}_i, \\\\mathbf{V}_i) = \\\\text{softmax} \\\\left( \\\\frac{\\\\mathbf{Q}_i \\\\mathbf{K}_i^\\\\top}{\\\\sqrt{d_k}} \\\\right) \\\\mathbf{V}_i\\n   \\\\]\\n\\n4. **Multi-Head Attention:**\\n   \\\\[\\n   \\\\text{MultiHeadAttention}(\\\\mathbf{Q}, \\\\mathbf{K}, \\\\mathbf{V}) = \\\\text{Concat}(\\\\text{head}_1, \\\\dots, \\\\text{head}_h) \\\\mathbf{W}^o\\n   \\\\]\\n\\n5. **Add & Norm:**\\n   \\\\[\\n   \\\\text{Add\\\\&Norm}(x, \\\\text{Sublayer}(x)) = \\\\text{LayerNorm}(x + \\\\text{Sublayer}(x))\\n   \\\\]\\n\\n6. **Feed-Forward Network:**\\n   \\\\[\\n   \\\\text{FFN}(x) = \\\\max(0, x \\\\mathbf{W}_1 + b_1) \\\\mathbf{W}_2 + b_2\\n   \\\\]\\n\\n7. **Encoder Layer:**\\n   \\\\[\\n   \\\\text{EncoderLayer}(x) = \\\\text{Add\\\\&Norm}(x, \\\\text{MultiHeadAttention}(x, x, x)) \\\\rightarrow \\\\text{Add\\\\&Norm}(x, \\\\text{FFN}(x))\\n   \\\\]\\n\\n8. **Decoder Layer:**\\n   \\\\[\\n   \\\\text{DecoderLayer}(y, \\\\text{encoder\\\\_output}) = \\\\text{Add\\\\&Norm}(y, \\\\text{MaskedMultiHeadAttention}(y, y, y)) \\\\rightarrow \\\\text{Add\\\\&Norm}(y, \\\\text{MultiHeadAttention}(y, \\\\text{encoder\\\\_output}, \\\\text{encoder\\\\_output})) \\\\rightarrow \\\\text{Add\\\\&Norm}(y, \\\\text{FFN}(y))\\n   \\\\]\\n\\n9. **Masked Multi-Head Attention:**\\n   \\\\[\\n   \\\\text{MaskedAttention}(\\\\mathbf{Q}_i, \\\\mathbf{K}_i, \\\\mathbf{V}_i) = \\\\text{softmax} \\\\left( \\\\frac{\\\\mathbf{Q}_i \\\\mathbf{K}_i^\\\\top}{\\\\sqrt{d_k}} + \\\\mathbf{M} \\\\right) \\\\mathbf{V}_i\\n   \\\\]\\n\\n10. **Final Linear and Softmax Layer:**\\n    \\\\[\\n    \\\\text{OutputProbs} = \\\\text{softmax}(\\\\text{Linear}(\\\\text{DecoderOutput}))\\n    \\\\]\\n\\nThis detailed breakdown with explanations should provide a comprehensive understanding of the transformer's mechanisms from end to end.\\n\\n\";"],"names":[],"sourceRoot":""}