## 1. What is an Environment in RL?

In reinforcement learning (RL), an environment is the external system in which an agent operates and interacts. But how do we formally define this concept? An environment in RL can be represented as a tuple:

$$ \mathcal{E} = (\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}) $$

Where:
- $\mathcal{S}$: Set of all possible states
- $\mathcal{A}$: Set of all possible actions
- $\mathcal{R}$: Reward function
- $\mathcal{P}$: Transition probability function

How does this environment definition relate to agents? Agents are:

$$ \mathcal{A} = (S, A, \pi, \gamma, L) $$

The environment $\mathcal{E}$ defines the rules and dynamics, while the agent $\mathcal{A}$ defines the decision-making entity. The agent operates within the constraints set by the environment.

Now that we understand the basic structure of an environment, let's delve deeper into each component.

## 2. States: The Snapshot of the Environment

What exactly is a state in RL? The state space $\mathcal{S}$ represents all possible situations the environment can be in:

$$ \mathcal{S} = \{s_1, s_2, s_3, \ldots\} $$

Each state $s \in \mathcal{S}$ provides a complete description of the environment at a particular moment in time. It includes all the information necessary for the agent to make a decision.

But this raises an important question: Can the agent always perceive the complete state of the environment?

I think s in S should be s_i in S right?

Can I get an example or analogy for the difference between state and environment?

Is state basically the full environment at a particular time t?

## 3. Observations: The Agent's Window to the World

In many real-world scenarios, the agent cannot directly access the full state of the environment. Instead, it receives an observation $o$, which is partial information about the environment's state:

$$ o = f(s), \text{ where } o \subseteq s \text{ and } s \in \mathcal{S} $$

Here, $f$ is a function that maps the true state to an observation, potentially losing information in the process.

This distinction between states and observations leads us to an important concept in RL: the difference between the true environment and the agent's perception.

Why is o lowercase? Is it because just like we have s_i in S we have o_i in O where O is a subset of S?

Make the function consistent with full RL defn.

## 4. The True Environment vs. Agent's Perception

How does what the agent perceives differ from the actual state of the environment? This concept is similar to the distinction between $y$ (true value) and $\hat{y}$ (estimated value) in supervised learning or statistics.

### True Environment (y):
$$ \mathcal{E} = (\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}) $$

### Agent's Perception (Å·):
- Observations: $o = f(s)$, where $s \in \mathcal{S}$
- Learned World Model: $M = (\hat{\mathcal{S}}, \mathcal{A}, \hat{\mathcal{R}}, \hat{\mathcal{P}})$

The agent must learn and make decisions based on its observations and learned model, which may not perfectly reflect the true environment.


This could use an analogy, example, more explanation.

But how does the agent interact with this environment over time?

## 5. Actions: The Agent's Toolkit

The action space $\mathcal{A}$ represents all possible actions an agent can take:

$$ \mathcal{A} = \{a_1, a_2, a_3, \ldots\} $$

Actions can be:
1. Discrete: $\mathcal{A} = \{a_1, a_2, a_3, \ldots\}$
2. Continuous: $\mathcal{A} = \{a \in \mathbb{R}^n \mid a_{\text{min}} \leq a \leq a_{\text{max}}\}$

The nature of the action space significantly impacts how the agent learns and makes decisions. But what happens after the agent takes an action?

## 6. Transitions: The Dynamics of the Environment

The transition probability function $\mathcal{P}$ defines how the environment changes in response to the agent's actions:

$$ \mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0, 1] $$

It gives the probability of moving from one state to another given an action. Transitions can be:

1. Deterministic: $s_{t+1} = f(s_t, a_t)$
2. Stochastic: $s_{t+1} \sim \mathcal{P}(\cdot | s_t, a_t)$

Here, $s_{t+1}$ (also sometimes denoted as $s'$) represents the next state after taking action $a_t$ in state $s_t$.

But how does the agent know if its actions are good or bad?

Why are transitions probabilistic?

## 7. Rewards: The Feedback Mechanism

The reward function $\mathcal{R}$ provides feedback to the agent about its actions:

$$ \mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R} $$

It assigns a numerical value to the transition from one state to another after taking an action:

$$ r_t = \mathcal{R}(s_t, a_t, s_{t+1}) $$

This feedback is crucial for the agent to learn which actions are beneficial and which are not.

Now that we understand the basic components of an environment, how do they work together over time?

## 8. Trajectory and History: Capturing the Agent's Journey

As the agent interacts with the environment, it generates a sequence of states, actions, and rewards. This sequence can be viewed from two perspectives:

### Trajectory:
A trajectory $\tau$ represents a specific sequence of true states and actions:
$$ \tau = (s_0, a_0, s_1, a_1, \ldots, s_T) $$

### History:
History $h_t$ captures the sequence of observations, actions, and rewards up to time $t$:
$$ h_t = (o_0, a_0, r_1, o_1, a_1, r_2, \ldots, o_t) $$

The key difference is that trajectory deals with true states (often unknown to the agent), while history deals with observations (what the agent actually perceives).

Is this distinction between trajectory and history actually true? Or how these words are commonly used in literature?

But how does the past influence the future in RL environments?

# memory

If we have a history, we need some way to represent what we/the agent remembers and what it doesn't. The agent can internally keep a track of the history or just infer it from the environment through observation.

## 9. The Markov Property and Its Implications

The Markov property states that the future depends only on the current state, not on the history:

$$ P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_1, a_1, s_2, a_2, \ldots, s_t, a_t) $$

This property leads to two important types of decision processes:

What is a decision process?


### Markov Decision Processes (MDPs):
In fully observable MDPs, the current observation (which equals the true state) is sufficient for optimal decision-making.

### Partially Observable Markov Decision Processes (POMDPs):
In POMDPs, the true state is Markovian, but the observations are not. The agent must use history to infer the current state, often through a belief state:

$$ b(s) = P(s | h_t) $$

How does this distinction affect the agent's decision-making process?

## 10. The Role of History in Different Settings

### In MDPs:
Theoretically, the current state is sufficient, and history is not needed for optimal decision-making. However, history can still be useful for learning and updating the policy or value function.

### In POMDPs:
History is crucial for maintaining a belief state and making informed decisions. The agent uses history to approximate the true state:

$$ \hat{s}_t = g(h_t) $$

Where $g$ is a function that estimates the current state based on history, often implemented through recurrent neural networks or other memory mechanisms.

But how can an agent use its experiences to build a model of the environment?

## 11. World Models: Bridging the Gap

The world model $M = (\hat{\mathcal{S}}, \mathcal{A}, \hat{\mathcal{R}}, \hat{\mathcal{P}})$ is the agent's learned approximation of the true environment $\mathcal{E}$.

### Learning the World Model:
The agent uses its history of interactions to learn the world model:

$$ \hat{\mathcal{P}}(s_{t+1}|s_t,a_t) \approx \frac{\text{count}(s_t, a_t, s_{t+1})}{\text{count}(s_t, a_t)} $$
$$ \hat{\mathcal{R}}(s_t, a_t) \approx \frac{\sum_{(s_t, a_t, r_t) \in \text{history}} r_t}{\text{count}(s_t, a_t)} $$

### Using the World Model:
The learned model allows the agent to simulate trajectories and plan actions without interacting with the true environment.

How do all these components come together in the agent's decision-making process?

## 12. Connecting the Dots: From Observations to Decision-Making

1. The agent receives an observation $o_t = f(s_t)$ of the true state $s_t$.
2. It updates its history: $h_t = h_{t-1} \cup (o_t, a_{t-1}, r_t)$.
3. In POMDPs, it updates its belief state: $b(s) = P(s | h_t)$.
4. It uses its policy $\pi$, which may depend on the history or belief state, to choose an action: $a_t = \pi(h_t)$ or $a_t = \pi(b(s))$.
5. The environment transitions to a new true state according to $\mathcal{P}(s_{t+1}|s_t, a_t)$.
6. The cycle repeats, generating a trajectory in the true environment and a history from the agent's perspective.

## The Interplay of Environments and Agents

Understanding RL environments involves grasping the distinction between the true environment and the agent's perception and learning. The concepts of states, observations, actions, transitions, rewards, history, and trajectories are all interconnected, each playing a crucial role in how the agent learns and makes decisions in the face of uncertainty and partial observability.

Different types of agents (value-based, policy-based, model-based) are designed to excel in different types of environments (fully observable, partially observable, deterministic, stochastic). The environment provides the context, rules, and feedback, while the agent develops strategies to maximize its performance within these constraints.

By understanding these connections, we can see how the framework of RL environments provides the foundation for developing intelligent agents capable of learning and adapting in complex, dynamic settings.

Tags:  [[Game Theory]], [[Information Theory]],  [[Artificial Intelligence]]



Markov assumption/Property
\[ P(s_{t+1} | h_t, a_t) = P(s_{t+1} | s_t, a_t) \] where \( h_t \) is the history up to time \( t \), \( s_t \) is the current state, \( a_t \) is the action taken, and \( s_{t+1} \) is the next state.

AI that learns how to make decisions by performing actions and receiving feedback in the form of rewards or punishments.

What are state visit distributions?
What is a visit distribution function?
What is a POMDP?

State transition matrix:
A state transition matrix in reinforcement learning is a table that describes the probabilities of transitioning from one state to another given a particular action. Each row represents a current state, each column represents a possible next state, and each cell contains the probability of moving to the next state from the current state when an action is taken. This matrix helps in modeling the dynamics of the environment, allowing for the prediction of future states based on current actions.

State transition matrices can be predefined in environments with known dynamics, or they can be learned from interactions with the environment in more complex or unknown settings. The elements of the matrix are probabilities, not weights or learned parameters in the traditional sense of neural network weights. However, in environments where the dynamics are unknown or too complex to model a priori, these probabilities can be estimated through experience, effectively learning the transition matrix through observation of state transitions that result from actions taken by the agent.

Initially, this matrix might come from domain knowledge about the environment, defining how actions lead to state transitions based on the rules or physics of the environment. In learning scenarios, especially in model-based reinforcement learning, the matrix is derived from the agent's interactions and observations, gradually refining the probability estimates as more data is collected. Thus, while the matrix itself doesn't contain learned parameters like those in a neural network, the process of estimating the probabilities can involve learning from data.

This involves learning the probabilities that describe how likely it is to transition from one state to another given a certain action in a reinforcement learning (RL) environment. It's about understanding the dynamics of the environmentâspecifically, predicting the next state based on the current state and action. This estimation helps in planning and decision-making within the RL framework. The learning here focuses on accurately modeling the environment's dynamics rather than directly improving the decision-making policy of the agent.


Model-Based RL: Model-based reinforcement learning involves learning a model of the environment's dynamics (how actions change the state of the environment) and rewards. The agent uses this model to simulate outcomes of different actions and plan the best course of action, effectively allowing it to anticipate future states and rewards to make informed decisions.

Model-Free RL: Model-free reinforcement learning directly learns the optimal policy or value function without constructing an explicit model of the environment's dynamics. The agent learns from the actual experience by interacting with the environment, relying on trial and error to figure out the best actions.

Online Learning: In online learning, the agent learns directly from the environment in a sequential and continuous manner, updating its knowledge with each new piece of information or interaction. This approach allows the agent to adapt to changes in the environment but requires access to the environment for continuous learning.

Offline Learning: Offline learning, also known as batch learning, involves training the agent on a fixed dataset of experiences collected beforehand, without further interaction with the environment during the learning process. This method is useful when direct interaction with the environment is costly, risky, or impractical, but it may limit the agent's ability to adapt to new or unseen situations.

 - Policy: A policy is a strategy used by the agent, defining which action to take in each state of the environment. It maps states to actions, guiding the agent's behavior.
 - Value Function: The value function estimates the total amount of reward an agent can expect to accumulate over the future, starting from a specific state and following a particular policy. It helps in evaluating the desirability of states.
 - Reward Function: The reward function provides immediate feedback to the agent by assigning a score (reward) to each action's outcome in a particular state, indicating the short-term benefit of that action.
 - These three elements are interconnected: the policy aims to maximize the cumulative reward as estimated by the value function, which in turn is calculated based on the rewards assigned by the reward function.

 - State: A representation of the current situation or condition of the environment that the agent is in.
 - Action: Any decision or move made by the agent that can change the state.
 - Episode: A sequence of states, actions, and rewards that ends when a terminal state is reached or after a certain number of steps.
 - Discount Factor: A number between 0 and 1 used to reduce the value of future rewards, reflecting the preference for immediate rewards over future rewards.
 - Q-Learning: A model-free reinforcement learning algorithm that learns the value of an action in a particular state, using the Q-function, without requiring a model of the environment.
 - Temporal Difference (TD) Learning: A method in reinforcement learning that updates the value estimate based on the difference (or error) between consecutive value estimates.

 - Policy-Based Methods: Policy-based methods in reinforcement learning directly learn the policy that dictates the agent's actions without explicitly learning a value function. These methods optimize the policy by maximizing expected rewards and are particularly effective in continuous action spaces or when the policy is stochastic.
 - Value-Based Methods: Value-based methods focus on learning the value function, which estimates how good each state or state-action pair is, in terms of expected rewards. The policy is then derived indirectly from the value function, typically by choosing actions that lead to states with higher values. These methods excel in discrete action spaces and when the policy can be easily determined from the value function.


 === ENVIRONMENTS ===

 - History: This is the sequence of states, actions, and rewards experienced by the agent up to the current moment. It represents the agent's interaction with the environment over time, which can be crucial for making informed decisions in partially observable environments.
 - Markov Property: The Markov property signifies that the future state of the environment depends only on the current state and the action taken, not on the sequence of events that preceded it. This property simplifies the decision-making process by focusing only on the present state for predicting future outcomes.
 - Observation: An observation is what the agent perceives about the current state of the environment.
	 - Difference Between Observation and State: The state represents the true condition of the environment, potentially including information not directly perceivable by the agent. In contrast, an observation is the agent's perception of the state, which might be incomplete or noisy, especially in partially observable settings.
 - Fully observable: The agent has access to complete information about the current state of the environment at all times, allowing it to make fully informed decisions.
 - Partially Observable: The agent has limited information about the current state, making it challenging to make optimal decisions without additional strategies or information.
 - Deterministic: Every action taken by the agent leads to a predictable outcome, with no uncertainty involved.
 - Stochastic: The outcome of actions is uncertain, introducing randomness into the state transitions or rewards.
 - Static: The environment does not change while the agent is deliberating, ensuring stability in decision-making contexts.
 - Dynamic: The state of the environment can change while the agent is making decisions, requiring the agent to adapt quickly.
 - Discrete: The environment has a finite number of states and actions, making it more manageable but potentially limiting in complexity.
 - Continuous: States or actions can take on a range of values, offering a more realistic but computationally challenging setting.

=== MARKOV ===

A Markov Decision Process (MDP) is a mathematical framework used to describe a fully observable environment for decision-making where outcomes are partly random and partly under the control of a decision maker. It's defined by a set of states, a set of actions, a transition model (which describes what state we might move to next, given a current state and action), and a reward function (which gives feedback on the desirability of a state). MDPs are used to model and solve problems where you need to make a sequence of decisions to maximize some notion of cumulative reward.

A Markov game, also known as a stochastic game, extends MDPs to include multiple decision-makers or agents. In a Markov game, each agent's decisions not only affect the environment but also the outcomes and rewards of other agents. It incorporates the dynamics of MDPs but within a multiplayer setting, introducing competitive or cooperative elements where each agent aims to optimize their own rewards through their actions, given the state of the environment and considering the actions of other agents.


WAKER: Weighted Ac-
quisition of Knowledge across Environments for Robustness
https://arxiv.org/pdf/2306.09205.pdf

Doing model based reinforcement learning basically means that you are learning how to model the environment instead of learning how maximize a value/reward function, which implicity has a model of the environment. We are basically making this more expliciti. The advantage of learning world models explicitly, is that they are more robust. You can learn how the world works in general, and then that can be applied to any reward function down the line later. This is a more promising approach to generally capable agents.


- [[#1. What is an RL Agent?|1. What is an RL Agent?]]
- [[#2. The Agent's Perspective: State Space|2. The Agent's Perspective: State Space]]
- [[#3. The Agent's Toolkit: Action Space|3. The Agent's Toolkit: Action Space]]
- [[#4. The Agent's Strategy: Policy|4. The Agent's Strategy: Policy]]
- [[#5. Planning for the Future: Discount Factor|5. Planning for the Future: Discount Factor]]
- [[#6. The Learning Process|6. The Learning Process]]
- [[#7. Feedback Mechanism: Reward|7. Feedback Mechanism: Reward]]
- [[#8. Measuring Agent Performance|8. Measuring Agent Performance]]
- [[#9. Estimating Long-Term Value: Value Functions|9. Estimating Long-Term Value: Value Functions]]
- [[#10. Types of RL Agents|10. Types of RL Agents]]
	- [[#10. Types of RL Agents#10.1 Model-Based Agents|10.1 Model-Based Agents]]
	- [[#10. Types of RL Agents#10.2 Model-Free Agents|10.2 Model-Free Agents]]
	- [[#10. Types of RL Agents#10.3 Value-Based Agents|10.3 Value-Based Agents]]
	- [[#10. Types of RL Agents#10.4 Policy-Based Agents|10.4 Policy-Based Agents]]
	- [[#10. Types of RL Agents#10.5 Actor-Critic Agents|10.5 Actor-Critic Agents]]
	- [[#10. Types of RL Agents#10.6 On-Policy Agents|10.6 On-Policy Agents]]
	- [[#10. Types of RL Agents#10.7 Off-Policy Agents|10.7 Off-Policy Agents]]
- [[#11. Improving Agent Performance|11. Improving Agent Performance]]

## 1. What is an RL Agent?

An RL agent is an entity that interacts with an environment to learn optimal behavior. But what exactly constitutes an agent? Let's break it down mathematically:

$$ A = (S, A, \pi, \gamma) $$

Where:
- $S$: State Space
- $A$: Action Space
- $\pi$: Policy (learned)
- $\gamma$: Discount Factor

This tuple represents the core components of an agent. But what do each of these mean, and how do they work together?

## 2. The Agent's Perspective: State Space

The state space $S$ is the set of all possible situations the agent might encounter:

$$ S = \{s_1, s_2, s_3, \ldots, s_n\} $$

Each $s_i$ represents a unique state. Think of it as the agent's perception of its environment at any given moment. For example, in a chess game, a state would be the current configuration of all pieces on the board.

But how does the agent interact with its environment based on these states?

## 3. The Agent's Toolkit: Action Space

The action space $A$ is the set of all possible actions the agent can take:

$$ A = \{a_1, a_2, a_3, \ldots, a_m\} $$

Each $a_j$ represents a unique action. Continuing our chess example, an action would be moving a specific piece to a specific square.

Now that we know what the agent can perceive (states) and what it can do (actions), how does it decide which action to take in each state?

## 4. The Agent's Strategy: Policy

The policy $\pi$ is the agent's strategy, mapping states to actions:

$\pi: S \rightarrow A$

It can be:
1. Deterministic: $a_t = \mu(s_t)$
2. Stochastic: $a_t \sim \pi(\cdot | s_t)$

A deterministic policy always chooses the same action for a given state, while a stochastic policy assigns probabilities to actions.

But why would an agent choose one action over another? This brings us to the concept of rewards and long-term planning.

## 5. Planning for the Future: Discount Factor

The discount factor $\gamma$ determines how much the agent values future rewards compared to immediate ones:

$$ 0 \leq \gamma \leq 1 $$

A higher $\gamma$ means the agent is more forward-thinking, valuing future rewards more heavily. This is crucial because in many scenarios, the best long-term strategy might involve sacrificing immediate rewards for greater future gains.

Now that we understand the basic components of an agent, how does it actually learn and improve its behavior?

## 6. The Learning Process

Agents learn through interaction with their environment. This process is guided by a learning algorithm (L). We can expand our agent tuple to include this:

$$ A = (S, A, \pi, \gamma, L) $$

But what does it mean for an agent to "learn"? In essence, learning in RL typically involves updating the policy $\pi$ or estimating value functions (which we'll discuss later). This often translates to adjusting parameters in neural networks or other function approximators.

Different types of agents use different learning algorithms. For example, in Q-learning:

$$ Q(s, a) \leftarrow Q(s, a) + \alpha [R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(s, a)] $$

This update rule adjusts the estimated value of taking action $a$ in state $s$ based on the reward received and the estimated value of the next state.

But how does the agent know if it's doing well or poorly? This brings us to the concept of rewards.

## 7. Feedback Mechanism: Reward

To learn, agents need feedback. This comes in the form of a reward function:

$$ R: S \times A \times S \rightarrow \mathbb{R} $$

The reward function assigns a numerical value to the transition from one state to another after taking an action:

$$ r_t = R(s_t, a_t, s_{t+1}) $$

This is how the environment communicates to the agent whether its actions are good or bad.

Given this feedback mechanism, how do we measure the overall performance of an agent?

## 8. Measuring Agent Performance

We typically measure agent performance by the expected cumulative reward:

$$ \text{Performance}(A) = \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t R_t \right] $$

We use the expected value because the environment and/or the agent's policy might be stochastic, leading to different outcomes even with the same starting conditions. We sum the rewards because we're interested in the agent's performance over time, not just in a single moment.

But how does the agent estimate the long-term value of its actions? This brings us to the concept of value functions.

## 9. Estimating Long-Term Value: Value Functions

Agents often learn value functions to estimate the long-term reward of being in a state or taking an action:

1. State-Value Function: $V: S \rightarrow \mathbb{R}$
   $$ V(s) = \mathbb{E}[G_t | S_t = s] = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Bigg| S_t = s \right] $$

2. Action-Value Function: $Q: S \times A \rightarrow \mathbb{R}$
   $$ Q(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a] = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Bigg| S_t = s, A_t = a \right] $$

The key difference is that the state-value function $V(s)$ estimates the expected return from being in state $s$ and following the current policy thereafter, while the action-value function $Q(s,a)$ estimates the expected return from taking action $a$ in state $s$ and then following the current policy.

Now that we understand these fundamental concepts, let's explore different types of RL agents and how they approach the learning problem.

## 10. Types of RL Agents

RL algorithms can be categorized in several ways. One fundamental distinction is between model-based and model-free methods:

### 10.1 Model-Based Agents

These agents learn a model of their environment:

$$ A_{mb} = (S, A, \hat{R}, \hat{P}, \pi: S \rightarrow A, \gamma, L_{mb}, PA) $$

Where:
- $\hat{R}: S \times A \times S \rightarrow \mathbb{R}$: Estimated Reward Function
- $\hat{P}: S \times A \times S \rightarrow [0, 1]$: Estimated Transition Probability Function
- $PA$: Planning Algorithm

Model-based agents try to learn how the environment works ($\hat{P}$ and $\hat{R}$) and use this knowledge to plan their actions.

### 10.2 Model-Free Agents

These agents learn directly from experience without building an explicit model:

$$ A_{mf} = (S, A, \pi: S \rightarrow A, \gamma, L_{mf}) $$

Model-free agents focus on learning values or policies directly from experience, without trying to model the environment's dynamics.

Another important distinction is between value-based and policy-based methods:

### 10.3 Value-Based Agents

These agents learn a value function (either V or Q) and derive their policy from it:

$$ A_{value} = (S, A, \pi: S \rightarrow A, \gamma, L_{value}, V: S \rightarrow \mathbb{R}) $$
or
$$ A_{value} = (S, A, \pi: S \rightarrow A, \gamma, L_{value}, Q: S \times A \rightarrow \mathbb{R}) $$

### 10.4 Policy-Based Agents

These agents directly learn a policy without necessarily learning a value function:

$$ A_{policy} = (S, A, \pi: S \rightarrow A, \gamma, L_{policy}) $$

### 10.5 Actor-Critic Agents

These combine aspects of both value-based and policy-based methods:

$$ A_{ac} = (S, A, \pi, \gamma, \pi_{actor}: S \rightarrow A, V_{critic}: S \rightarrow \mathbb{R}, L_{ac}) $$

Actor-Critic methods maintain both a policy (the actor) and a value function (the critic), using the critic to update the actor.

Finally, we can distinguish between on-policy and off-policy methods:

### 10.6 On-Policy Agents

These agents learn about the policy they're currently following:

$$ A_{on\_policy} = (S, A, \pi: S \rightarrow A, \gamma, L_{on\_policy}) $$

### 10.7 Off-Policy Agents

These agents can learn about a policy different from the one they're following:

$$ A_{off\_policy} = (S, A, \pi: S \rightarrow A, \gamma, L_{off\_policy}) $$

Understanding these different types of agents and algorithms is crucial for choosing the right approach for a given problem. But how can we improve the performance of these agents?

## 11. Improving Agent Performance

There are several strategies for enhancing agent performance:

1. Better Learning Algorithms: For example, Double Q-learning to reduce overestimation bias:

   $$ Q_1(s, a) \leftarrow Q_1(s, a) + \alpha [r + \gamma Q_2(s', \arg\max_{a'} Q_1(s', a')) - Q_1(s, a)] $$
   $$ Q_2(s, a) \leftarrow Q_2(s, a) + \alpha [r + \gamma Q_1(s', \arg\max_{a'} Q_2(s', a')) - Q_2(s, a)] $$

2. Experience Replay: Storing and reusing past experiences to improve sample efficiency.

3. Prioritized Experience Replay: Sampling important experiences more frequently:

   $$ P(i) = \frac{|\delta_i| + \epsilon}{\sum_k (|\delta_k| + \epsilon)} $$

   Where $P(i)$ is the probability of sampling experience $i$, $\delta_i$ is the TD error of experience $i$, and $\epsilon$ is a small constant.

4. Function Approximation: Using neural networks to handle large state spaces.

5. Exploration Strategies: Balancing exploration and exploitation to discover optimal policies.

These techniques help address various challenges in RL, such as sample efficiency, stability of learning, and scalability to complex environments.



























An agent in Reinforcement Learning (RL) is something that
interacts with [[Environment]]
makes decisions to maximize cumulative rewards. 

You can think of it as a tuple:

\[ A = (S, A, \pi, \gamma) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor

Question: What is an agent's state space in RL?
Equation:
\[ A = (S, A, \pi, \gamma) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor

Question: What is an agent's action space in RL?
Equation:
\[ A = (S, A, \pi, \gamma) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor

Question: What is an agent's policy in RL?
Equation:
\[ A = (S, A, \pi: S \rightarrow A, \gamma) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor

For deterministic policies:
\[ a_t = \mu(s_t) \]

For stochastic policies:
\[ a_t \sim \pi(\cdot | s_t) \]

Question: What does an agent's learning algorithm do in RL?
Equation:
\[ A = (S, A, \pi: S \rightarrow A, \gamma, L) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( L \): Learning Algorithm

For example, in Q-learning, the update rule is:
\[ Q(s, a) \leftarrow Q(s, a) + \alpha [R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(s, a)] \]
where:
- \(Q(s, a)\) is the action-value function.
- \(\alpha\) is the learning rate.
- \(R_{t+1}\) is the reward received after taking action \(a\) in state \(s\).
- \(\gamma\) is the discount factor.
- \(S_{t+1}\) is the next state.
- \(a'\) is the next action.

Question: What is an agent's reward function in RL?
Equation:
\[ A = (S, A, R: S \times A \times S \rightarrow \mathbb{R}, \pi: S \rightarrow A, \gamma) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( R: S \times A \times S \rightarrow \mathbb{R} \): Reward Function
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor

The reward function can be defined as:
\[ r_t = R(s_t, a_t, s_{t+1}) \]

Question: What is a state-value based agent in RL?
Equation:
\[ A_{state\_value} = (S, A, \pi: S \rightarrow A, \gamma, L_{sv}, V: S \rightarrow \mathbb{R}) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( L_{sv} \): State-Value Learning Algorithm
- \( V: S \rightarrow \mathbb{R} \): State-Value Function (learned)

The state-value function \(V(s)\) can be formally defined as:
\[ V(s) = \mathbb{E}[G_t | S_t = s] = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Bigg| S_t = s \right] \]

Question: What is an action-value based agent in RL?
Equation:
\[ A_{action\_value} = (S, A, \pi: S \rightarrow A, \gamma, L_{av}, Q: S \times A \rightarrow \mathbb{R}) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( L_{av} \): Action-Value Learning Algorithm
- \( Q: S \times A \rightarrow \mathbb{R} \): Action-Value Function (learned)

The action-value function \(Q(s, a)\) can be formally defined as:
\[ Q(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a] = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Bigg| S_t = s, A_t = a \right] \]

Question: What is a memory-based agent in RL?
Equation:
\[ A_{memory} = (S, A, \pi: S \rightarrow A, \gamma, L_{memory}, M) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( L_{memory} \): Memory-Based Learning Algorithm
- \( M \): Memory Mechanism (e.g., experience replay, recurrent neural network)

The memory mechanism \(M\) can be used in various ways, such as experience replay in DQN, where the agent stores and samples past experiences:
\[ M = \{(s_t, a_t, r_{t+1}, s_{t+1})\} \]

Question: What is a model-based agent in RL?
Equation:
\[ A_{mb} = (S, A, \hat{R}, \hat{P}, \pi: S \rightarrow A, \gamma, L_{mb}, PA) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \hat{R}: S \times A \times S \rightarrow \mathbb{R} \): Estimated Reward Function (learned)
- \( \hat{P}: S \times A \times S \rightarrow [0, 1] \): Estimated Transition Probability Function (learned)
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( L_{mb} \): Model-Based Learning Algorithm
- \( PA \): Planning Algorithm

The model of the environment consists of the transition function \(\hat{P}\) and the reward function \(\hat{R}\):
\[ \hat{P}(s'|s, a) \approx P(s'|s, a) \]
\[ \hat{R}(s, a, s') \approx R(s, a, s') \]

Question:

 What is a model-free agent in RL?
Equation:
For a model-free agent using Q-learning:
\[ Q(s, a) \leftarrow Q(s, a) + \alpha [R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(s, a)] \]
where:
- \(Q(s, a)\) is the action-value function.
- \(\alpha\) is the learning rate.
- \(R_{t+1}\) is the reward received after taking action \(a\) in state \(s\).
- \(\gamma\) is the discount factor.
- \(S_{t+1}\) is the next state.
- \(a'\) is the next action.

Question: What is an on-policy agent in RL?
Equation:
\[ A_{on\_policy} = (S, A, \pi: S \rightarrow A, \gamma, L_{on\_policy}) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( L_{on\_policy} \): On-Policy Learning Algorithm

For example, in the SARSA (State-Action-Reward-State-Action) algorithm:
\[ Q(s, a) \leftarrow Q(s, a) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(s, a)] \]
where:
- \(Q(s, a)\) is the action-value function.
- \(\alpha\) is the learning rate.
- \(R_{t+1}\) is the reward received after taking action \(a\) in state \(s\).
- \(\gamma\) is the discount factor.
- \(S_{t+1}\) is the next state.
- \(A_{t+1}\) is the next action taken according to the current policy \(\pi\).

Question: What is an off-policy agent in RL?
Equation:
\[ A_{off\_policy} = (S, A, \pi: S \rightarrow A, \gamma, L_{off\_policy}) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( L_{off\_policy} \): Off-Policy Learning Algorithm

For example, in Q-learning:
\[ Q(s, a) \leftarrow Q(s, a) + \alpha [R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(s, a)] \]
where:
- \(Q(s, a)\) is the action-value function.
- \(\alpha\) is the learning rate.
- \(R_{t+1}\) is the reward received after taking action \(a\) in state \(s\).
- \(\gamma\) is the discount factor.
- \(S_{t+1}\) is the next state.
- \(a'\) is the action that maximizes \(Q\) in the next state \(S_{t+1}\).

Question: What is an actor-critic agent in RL?
Equation:
\[ A_{ac} = (S, A, \pi, \gamma, \pi_{actor}, V_{critic}, L_{ac}) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( \pi_{actor}: S \rightarrow A\): Actor (Policy Network) (learned)
- \( V_{critic}: S \rightarrow \mathbb{R}\): Critic (Value Function Network) (learned)
- \( L_{ac}\): Actor-Critic Learning Algorithm

In the Actor-Critic method, the update rules for the actor and critic are:

Critic Update (TD Error):
\[ \delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \]

Actor Update (Policy Gradient):
\[ \theta \leftarrow \theta + \alpha \delta_t \nabla_{\theta} \log \pi(a|s, \theta) \]
where:
- \(\delta_t\) is the temporal difference (TD) error.
- \(V(s)\) is the state-value function.
- \(\alpha\) is the learning rate.
- \(\theta\) are the parameters of the policy \(\pi\).
- \(\pi(a|s, \theta)\) is the probability of taking action \(a\) in state \(s\) under policy parameters \(\theta\).

Question: What is an online learning agent in RL?
Equation:
\[ A_{online} = (S, A, \pi: S \rightarrow A, \gamma, L_{online}) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( L_{online} \): Online Learning Algorithm

For example, in online Q-learning, the update occurs after each action:
\[ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] \]
where \(r\) is the reward received.

Question: What is an offline learning agent in RL?
Equation:
\[ A_{offline} = (S, A, \pi: S \rightarrow A, \gamma, L_{offline}, M) \]
- \( S = \{s_1, s_2, s_3, \ldots, s_n\} \): State Space
- \( A = \{a_1, a_2, a_3, \ldots, a_m\} \): Action Space
- \( \pi: S \rightarrow A \) : Policy (learned)
- \( \gamma \): Discount Factor
- \( L_{offline} \): Offline Learning Algorithm
- \( M \): Memory Mechanism for storing experiences

For example, in offline Q-learning with experience replay, the update occurs using a batch of experiences:
\[ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] \]
where the experiences \((s, a, r, s')\) are sampled from memory \(M\).

Question: How do we measure the performance of agents in RL?
Equation:
\[ \text{Performance}(A) = \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t R_t \right] \]

Equation:
- \( G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \): Expected cumulative reward
- \( \gamma \): Discount Factor

Question: How do we measure agent performance in RL (metrics)?
Equation:
\[ G_t = \sum_{t=0}^{T} R_t \]
where \(T\) is the total time horizon.

Question: How do we improve the performance of agents in RL?
Equation:
Example of enhancing a learning algorithm (Double Q-learning):
\[ Q_1(s, a) \leftarrow Q_1(s, a) + \alpha [r + \gamma Q_2(s', \arg\max_{a'} Q_1(s', a')) - Q_1(s, a)] \]
\[ Q_2(s, a) \leftarrow Q_2(s, a) + \alpha [r + \gamma Q_1(s', \arg\max_{a'} Q_2(s', a')) - Q_2(s, a)] \]

Question: How do we improve the learning efficiency of RL agents?
Equation:
Example of prioritized experience replay:
\[ P(i) = \frac{|\delta_i| + \epsilon}{\sum_k (|\delta_k| + \epsilon)} \]
where \(P(i)\) is the probability of sampling experience \(i\), \(\delta_i\) is the TD error of experience \(i\), and \(\epsilon\) is a small constant to ensure all experiences have a non-zero probability of being sampled.


